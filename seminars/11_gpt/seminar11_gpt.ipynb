{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3XPuaTu9jl"
   },
   "source": [
    "## 11. GPT\n",
    "\n",
    "## План\n",
    "1. GPT from scratch\n",
    "2. Transformer++\n",
    "3. Стратегии декодирования\n",
    "4. Constrained decoding с json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c9YO2vQyKOL"
   },
   "source": [
    "Датасет возьмём из известного репозитория Карпатого [nanoGPT](https://github.com/karpathy/nanoGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MLU3qnsAnKcU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/sergey/anaconda3/envs/work/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets==2.21.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (2.21.0)\n",
      "Requirement already satisfied: transformers in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (4.46.3)\n",
      "Requirement already satisfied: nltk in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sergey/.local/lib/python3.8/site-packages (from datasets==2.21.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (3.1.0)\n",
      "Requirement already satisfied: multiprocess in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (0.31.1)\n",
      "Requirement already satisfied: packaging in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from datasets==2.21.0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: click in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from aiohttp->datasets==2.21.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from requests>=2.32.2->datasets==2.21.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sergey/.local/lib/python3.8/site-packages (from requests>=2.32.2->datasets==2.21.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from requests>=2.32.2->datasets==2.21.0) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from pandas->datasets==2.21.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from pandas->datasets==2.21.0) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sergey/anaconda3/envs/work/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.21.0) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/sergey/anaconda3/envs/work/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/sergey/anaconda3/envs/work/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.21.0 transformers nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hBMNZi4Tnau1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "ddygj4zOlZAU",
    "outputId": "005be42d-3b50-4b75-8016-319f5f96bd3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/sergey/.cache/huggingface/modules/datasets_modules/datasets/karpathy--tiny_shakespeare/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e (last modified on Thu May 15 15:15:45 2025) since it couldn't be found locally at karpathy/tiny_shakespeare, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "text = datasets.load_dataset('karpathy/tiny_shakespeare')['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HLszWkZptxwb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4f42066f-fc46-44e7-872d-05c03bb6099d"
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(' '.join(text), language='english')\n",
    "train_sentences = sentences[:10000]\n",
    "test_sentences = sentences[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p2WuY4O_t68J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 993)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f3d44c6f-8b59-48d9-81d2-fe56eb120f40"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer(self.data[idx], return_tensors = 'pt', max_length = self.max_length, truncation=True).input_ids[0]\n",
    "        input_ids, labels = input_ids[:-1], input_ids[1:]\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7phijn9zUiel"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def data_collator(examples, padding_value=0, max_length=2048):\n",
    "    def trim_and_pad(seq, batch_first=True, padding_value=0):\n",
    "        return pad_sequence([s[:max_length] for s in seq], batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "    input_ids = trim_and_pad(\n",
    "        [example[\"input_ids\"] for example in examples],\n",
    "        batch_first=True,\n",
    "        padding_value=padding_value,\n",
    "    )\n",
    "    targets = trim_and_pad(\n",
    "        [example[\"labels\"] for example in examples],\n",
    "        batch_first=True,\n",
    "        padding_value=-100,\n",
    "    )\n",
    "    attention_mask = trim_and_pad(\n",
    "        [example[\"attention_mask\"] for example in examples],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": targets,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yq7XBN4xUiem"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "A5pKjRFUUiem"
   },
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tVwD1pRCUiem"
   },
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "train_dataset = CustomDataset(train_sentences, tokenizer, max_length=max_length)\n",
    "test_dataset = CustomDataset(test_sentences, tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle = True, collate_fn=partial(data_collator, padding_value=tokenizer.pad_token_id, max_length=max_length))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle = False, collate_fn=partial(data_collator, padding_value=tokenizer.pad_token_id, max_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HZmUQh4sUiem"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([3237,   25,  198, 5248,  461,   11, 2740]),\n",
       " 'labels': tensor([  25,  198, 5248,  461,   11, 2740,   13]),\n",
       " 'attention_mask': tensor([True, True, True, True, True, True, True])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uoBkRBolUiem"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rZ3_quwtUiem"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What\\nsayest thou to this tune, matter and method<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'O pity, pity, gentle heaven, pity<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"They have made worms' meat of me: I have it,\\nAnd soundly too: your houses<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'KING RICHARD III:\\nHoyday, a riddle<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"CAPULET:\\nSend for the county; go tell him of this:\\nI'll have this knot knit up to-morrow morning<|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'SICINIUS:\\nHave you a catalogue\\nOf all the voices that we have procured\\nSet down by the poll<|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"I'll be at charges for a looking-glass,\\nAnd entertain some score or two of tailors,\\nTo study fashions to adorn\",\n",
       " 'Whip me<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'come in: the wish deserves a welcome<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'I pray, sir, can you read<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'First Lady:\\nWhy, my sweet lord<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'LADY MONTAGUE:\\nThou shalt not stir a foot to seek a foe<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Either thou art most ignorant by age,\\nOr thou wert born a fool<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'HASTINGS:\\nI go, my lord<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'First Senator:\\nHe cannot but with measure fit the honours\\nWhich we devise him<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Which in his greatest need will shrink from him<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"MERCUTIO:\\n'Tis no less, I tell you, for the bawdy hand of the\\ndial is now upon the prick\",\n",
       " \"Second Murderer:\\nIn the Duke of Gloucester's purse<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'Good, good my lord, bethink you;\\nWho is it that hath died for this offence<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'My women, come; you have leave<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'CORIOLANUS:\\nShall remain<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"MARCIUS:\\nHere: what's the matter<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " \"'Do it presently<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'Mis-shapen chaos of well-seeming forms<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'PRINCE:\\nRomeo slew him, he slew Mercutio;\\nWho now the price of his dear blood doth owe<|endoftext|><|endoftext|>',\n",
       " \"ROMEO:\\n'Tis torture, and not mercy: heaven is here,\\nWhere Juliet lives; and every cat and dog\\nAnd little mouse\",\n",
       " 'JOHN OF GAUNT:\\nO, but they say the tongues of dying men\\nEnforce attention like deep harmony:\\nWhere words are scarce,',\n",
       " 'GREEN:\\nMy comfort is that heaven will take our souls\\nAnd plague injustice with the pains of hell<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'Give me a case to put my visage in:\\nA visor for a visor<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'No, no, my heart will burst, and if I speak:\\nAnd I will speak, that so my heart may burst<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'O miserable thought<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'You are the fount that makes small brooks to flow:\\nNow stops thy spring; my sea sha$l suck them dry,\\nAnd',\n",
       " 'O friar, the damned use that word in hell;\\nHowlings attend it: how hast thou the heart,\\nBeing a divine, a ghost',\n",
       " 'LARTIUS:\\nHe did, my lord<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'RATCLIFF:\\nThomas the Earl of Surrey, and himself,\\nMuch about cock-shut time, from troop to troop\\nWent through',\n",
       " 'Is not his heir a well-deserving son<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'KING RICHARD III:\\nWell,\\nGo muster men; but, hear you, leave behind\\nYour son, George Stanley: look your faith',\n",
       " 'FRIAR LAURENCE:\\nHoly Saint Francis, what a change is here<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"CORIOLANUS:\\nThe fires i' the lowest hell fold-in the people<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'it is too rough,\\nToo rude, too boisterous, and it pricks like thorn<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'CAMILLO:\\nAy, my good lord<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'CAMILLO:\\nMy lord,\\nFear none of this: I think you know my fortunes\\nDo all lie there: it shall be so',\n",
       " 'LEONTES:\\nOnce more, take her hence<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"Shepherd:\\nLet's before as he bids us: he was provided to do us good<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'First Murderer:\\nDo so, it is a point of wisdom: fare you well<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'CLARENCE:\\nThy voice is thunder, but thy looks are humble<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'CORIOLANUS:\\nMust I go show them my unbarbed sconce<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"NORFOLK:\\nWe'll all assist you; he that flies shall die<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " \"BENVOLIO:\\nHe ran this way, and leap'd this orchard wall:\\nCall, good Mercutio<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'I think most understand\\nBohemia stays here longer<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"What is't I dream on<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " \"What's their seeking<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'Know man from man<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"POLIXENES:\\nWhat is the news i' the court<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " 'Alas<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'DUKE OF AUMERLE:\\nIf God prevent not, I purpose so<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'First Gentleman:\\nWho would be thence that has the benefit of access<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " \"Then, in God's name, march:\\nTrue hope is swift, and flies with swallow's wings:\\nKings it makes gods, and meaner\",\n",
       " \"or those doves' eyes,\\nWhich can make gods forsworn<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n",
       " \"Be opposite all planets of good luck\\nTo my proceedings, if, with pure heart's love,\\nImmaculate devotion, holy thoughts,\\nI\",\n",
       " 'LEONTES:\\nProceed:\\nNo foot shall stir<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'PRINCE EDWARD:\\nAnd take his thanks that yet hath nothing else<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'That I may call thee something more than man\\nAnd after that trust to thee<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " 'to your corrected son<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ntxzOOxxUiem"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  198, 16706,   395, 14210,   284,   428, 14009,    11,  2300,   290,\n",
       "          2446,    30,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100],\n",
       "        [26246,    11, 26246,    11, 10296,  9538,    11, 26246,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100],\n",
       "        [  423,   925, 29174,     6,  6174,   286,   502,    25,   314,   423,\n",
       "           340,    11,   198,  1870,  2128,   306,  1165,    25,   534,  7777,\n",
       "             0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100],\n",
       "        [  371, 20739,  9795,  6711,    25,   198,    39,   726,   820,    11,\n",
       "           257,   374,  2509,     0,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100],\n",
       "        [24212,    51,    25,   198, 25206,   329,   262,  7968,    26,   467,\n",
       "          1560,   683,   286,   428,    25,   198,    40,  1183,   423,   428,\n",
       "         29654, 30495,   510,   284,    12,  9201,  3329,    13,  -100,  -100,\n",
       "          -100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x_-kuc7iUiem"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  9, 21, 14, 28, 27, 31,  3,  8,  8,  9, 19, 16, 10, 18,  9, 31, 13,\n",
       "        20,  8, 10, 11,  4, 12, 29, 31, 31, 21, 19, 26,  3, 31, 31, 11, 31, 10,\n",
       "        31, 17, 19, 20, 11, 31, 11, 19, 18, 16, 18, 17, 26, 11,  6,  4,  4, 14,\n",
       "         2, 18, 14, 31, 15, 31, 13, 16, 16,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"attention_mask\"].sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKgvxBWTUien"
   },
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mhi1HEcUoIF"
   },
   "source": [
    "## Positional Encoding\n",
    "В оригинальном [Трансформере](https://arxiv.org/abs/1706.03762) предложили абсолютные периодические позиционные эмбеддинги:\n",
    "\n",
    "$$\\text{emb}(p, 2i) = \\sin(\\frac{p}{10000^{2i/d}})$$\n",
    "$$\\text{emb}(p, 2i + 1) = \\cos(\\frac{p}{10000^{2i/d}})$$\n",
    "где $p, 2i, 2i+1$ - индексы элемента последовательности, $d$ - длина последовательности.\n",
    "Они плохо шкалируются на длины за границей тренировочных данных, поэтому появилось много других методов, рассмотрим далее ставший стандартом [RoPE](https://arxiv.org/abs/2104.09864)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d6116605-1c11-453e-9043-5dead9dce59c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]) # (batch, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoQjIeqZUien"
   },
   "source": [
    "## Слой аттеншена, a.k.a. One-Head Scaled Dot-Product Self-Attention\n",
    "\n",
    "- Каждый элемент выборки $X$ - последовательность фичей $x_i \\in \\mathbb{R}^D$ переменной длины: $X = \\{x_1, x_2, \\ldots, x_{n_i}\\}$\n",
    "- То есть $X$ это матрица: $X \\in \\mathbb{R}^{n_i \\times D}$\n",
    "- Параметры слоя - матрицы фиксированного размера: $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times d}$\n",
    "- Вычисление:\n",
    "    - $Q = X W_Q$ размера $n_i \\times d_k$\n",
    "    - $K = X W_K$ размера $n_i \\times d_k$\n",
    "    - $V = X W_V$ размера $n_i \\times d_v$\n",
    "- $Q, K$ можно нормализовать\n",
    "\n",
    "## Attention это \"мягкий словарь\"\n",
    "\n",
    "- `{key1: value1, key2: value2, ...}`\n",
    "- **Query** - запрос на поиск среди **Key**\n",
    "- Пусть $q_i$ и $k_j$ нормализованы: $\\|q_i\\| = \\|k_j\\| = 1$\n",
    "- Тогда $\\alpha_{ij}=\\langle q_i, k_j \\rangle = \\cos(\\theta)$, где $\\theta$ - угол между $q_i$ и $k_j$, можно мерить косинусную схожесть\n",
    "- Мы хотим померить релевантность всех ключей $k_j, j \\in [d]$ запросу $q_i$\n",
    "- Ответом на $i$-тый запрос будет вернуть линейную комбинацию всех **Value** с найденными скорами релевантности $\\alpha_{ij}$\n",
    "\n",
    "<img src=\"attention_example.svg\" style=\"width:75%\">\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\operatorname{Row-Wise\\ Softmax}\\left(\\dfrac{Q K^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$   \n",
    "\n",
    "*Почему такая нормализация?*\n",
    "$$\n",
    "q \\sim \\mathrm{N}(0, \\sigma^2), k \\sim \\mathrm{N}(0, \\sigma^2) \\rightarrow \\mathbb{V} \\left[ \\sum_{i=1}^{d_k} q_i k_i\\right] = \\sum_{i=1}^{d_k} \\mathbb{V} \\left[  q_i k_i\\right] = \\sum_{i=1}^{d_k} \\mathbb{V}\\left[  q_i\\right]\\mathbb{V} \\left[k_i\\right]= \\sigma^4 d_k\n",
    "$$\n",
    "Стандартное отклонение будет пропорционально $\\sigma \\sqrt{d_k}$, отсюда нормализация.\n",
    "\n",
    "Если мы не уменьшим дисперсию обратно до $\\sigma^2$, то софтмакс по логитам уже насытится до $1$ для одного случайного элемента и $0$ для всех остальных. Градиенты через softmax затухнут, так что мы не сможем выучить параметры должным образом.\n",
    "\n",
    "На $\\sigma$ мы не масштабируем, так как при инициализации весов обычно $\\sigma << 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PKxYZC6Uien"
   },
   "source": [
    "### Multi-Head Attention (MHA)\n",
    "<img src=\"mha_img_original.png\" style=\"width:75%\">\n",
    "\n",
    "- Разделяем каждый вектор из $Q, K, V$ на `num_heads` подвекторов с помощью идентичных линейных проекций $h$ раз ($D \\mod h = 0$)\n",
    "- Применяем Attention независимо, соединяем результаты\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Multihead}(Q, K, V) = \\textrm{concat} \\left( \\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h \\right) W_O,\\\\\n",
    "\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i), i = [h]; W_O \\in \\mathbb{R}^{h d_v \\times d_{out}}\n",
    "\\end{align*}\n",
    "$$\n",
    "- **Можно ли написать без цикла?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "99a20313-cd2c-4d46-8f7c-5745515db13d"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_bias = True):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model is not divisible by h\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.query = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.key = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.value = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.scale = self.d_head ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        query = self.query(x).view(B, L, self.n_heads, self.d_head).permute(0, 2, 1, 3)\n",
    "        key = self.key(x).view(B, L, self.n_heads, self.d_head).permute(0, 2, 1, 3)\n",
    "        value = self.value(x).view(B, L, self.n_heads, self.d_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        dots = (query @ key.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        mask = torch.tril(torch.ones((L, L))).to(x.device)\n",
    "        dots.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "        att_scores = dots.softmax(-1)\n",
    "        att_v = att_scores @ value\n",
    "\n",
    "        out = att_v.permute(0, 2, 1, 3).contiguous().view(B, L, D)\n",
    "\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bead0cea-cbec-4cab-a838-f8f4ca447e21"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_bias = True):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.MHA = MultiHeadAttention(d_model, n_heads, qkv_bias)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d_model, 4 * d_model), nn.ReLU(), nn.Linear(4 * d_model, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer_norm_1(self.MHA(x)) + x\n",
    "        x = self.layer_norm_2(self.MLP(x)) + x\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "e0b8a820-71e8-4b62-87a2-82d283deae8d"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_blocks, qkv_bias = True, seq_len = 2048):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, seq_len)\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(d_model, n_heads, qkv_bias) for _ in range(n_blocks)])\n",
    "        self.final_ln = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(next(iter(self.parameters())).device)\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.pos(emb)\n",
    "\n",
    "        x = self.blocks(emb)\n",
    "\n",
    "        out = self.lm_head(self.final_ln(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "f3e66a23-38f8-42bf-b9ba-0e523c89559a"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "#DEVICE = \"mps\"\n",
    "D_MODEL = 512\n",
    "N_HEADS = 8\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "N_BLOCKS = 3\n",
    "mha = MultiHeadAttention(D_MODEL, N_HEADS).to(DEVICE)\n",
    "decoder_block = DecoderBlock(D_MODEL, N_HEADS).to(DEVICE)\n",
    "model = TransformerDecoder(VOCAB_SIZE, D_MODEL, N_HEADS, N_BLOCKS).to(DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "284197cc-8cae-43fc-aa87-0c7071e65ba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey/anaconda3/envs/work/lib/python3.8/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/sergey/anaconda3/envs/work/lib/python3.8/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "loss_fnc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lczrh3GjvGUe"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0d076cd-ea69-470c-aa96-18d114ab4b65",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92184bcd9ab3444bab5e5b21b95d0978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bcf56ffe794116abf87f267b348cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabb6908fe6b4fe1b6b6e2ac7af2e7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_epoch 1: 5.40752817870705\n",
      "Loss_eval 1: 5.066189527511597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925e329b99024170b88ac80e1b72be56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107021526561420594a940c12c486711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_epoch 2: 4.211929696380713\n",
      "Loss_eval 2: 4.906144082546234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f2fb76252741d3be842cb879810ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52583fdafa2245f9b6efa6f1f7227754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_epoch 3: 3.757310858198032\n",
      "Loss_eval 3: 4.9725475907325745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ead5f69d8461391e7aa2266ef544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671abbbebcfb43b48f80a0ccf60ec119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_epoch 4: 3.4166673292779617\n",
      "Loss_eval 4: 5.072928071022034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d8d0406921462d9f1c25a45c89a1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_epochs = []\n",
    "loss_evals = []\n",
    "for epoch in tqdm(range(10)):\n",
    "    loss_epoch = []\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        labels_ids = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        preds = model(input_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fnc(preds.view(-1, preds.size(-1)), labels_ids.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "    loss_eval = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            labels_ids = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            preds = model(input_ids)\n",
    "\n",
    "            loss = loss_fnc(preds.view(-1, preds.size(-1)), labels_ids.view(-1))\n",
    "            loss_eval.append(loss.item())\n",
    "\n",
    "    print(f'Loss_epoch {epoch + 1}: {np.mean(loss_epoch)}')\n",
    "    print(f'Loss_eval {epoch + 1}: {np.mean(loss_eval)}')\n",
    "    loss_epochs.append(np.mean(loss_epoch))\n",
    "    loss_evals.append(np.mean(loss_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebb11869-5683-46b4-80be-b0eebdae1cb1"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_epochs, label=\"train\")\n",
    "plt.plot(loss_evals, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c9c5504-b1e5-4e54-9cfe-9dc4db6e975d"
   },
   "outputs": [],
   "source": [
    "batch = train_dataset[3]\n",
    "input_ids, labels_ids = batch[\"input_ids\"], batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "231d0318-89cb-4d1f-b711-0dd88a17cc25"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(model, ids, is_decode = False, max_length = 20):\n",
    "    model.eval()\n",
    "    if type(ids) == list:\n",
    "        symbols = torch.tensor(ids, device = DEVICE).view(len(ids),1)\n",
    "\n",
    "    else:\n",
    "        symbols = torch.tensor(ids, device = DEVICE).view(1,1)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "\n",
    "        cur_symbols = model(symbols).argmax(-1)[:, -1][:, None]\n",
    "        symbols = torch.cat([symbols, cur_symbols], dim = 1)\n",
    "\n",
    "    if is_decode:\n",
    "        symbols = tokenizer.batch_decode(symbols, skip_special_tokens=True)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6482aa45-881d-45d5-9b94-792a770cb211"
   },
   "outputs": [],
   "source": [
    "generate_batch(model, ids = input_ids[0].item(), is_decode=True, max_length= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f364c43-a851-42dc-8053-9784ae28688e"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(labels_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmx1Et3DUiep"
   },
   "source": [
    "# Transformer++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7i76daXW33H"
   },
   "source": [
    "## RMSNorm\n",
    "Первое архитектурное нововведение - исключение обучаемого `bias` из нормализации. Делает обучение быстрее и стабильнее благодаря своей простоте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4df2039-5606-4417-b844-f594b7365fd8"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9QG3nVlXLZu"
   },
   "source": [
    "## RoPE\n",
    "<img src=\"Screen_Shot_2021-08-10_at_10.38.41_AM.png\" style=\"width:75%\">\n",
    "\n",
    "RoPE (Rotary Position Embedding) — это способ кодирования позиционной информации в трансформерах, который использует вращательные преобразования для улучшения моделирования отношений между токенами. Вместо прибавления позиционных эмбеддингов к токенам, RoPE вращает их, учитывая относительную позицию токенов в последовательности. Для пары токенов $x_i, x_j$ фазовая разница учитывается напрямую через скалярное произведение, сохраняя относительное расположение токенов:\n",
    "\n",
    "$$\n",
    "\\operatorname{RoPE}(x) = \\begin{bmatrix} x^{(1)} \\cos \\theta - x^{(2)} \\sin \\theta \\\\ x^{(1)} \\sin \\theta + x^{(2)} \\cos \\theta \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "\n",
    "где $\\theta=\\text{freq}(i)$ определяется в зависимости от позиции $i$ и масштабируется для каждой частоты. Такой подход позволяет трансформеру эффективно обрабатывать как локальные, так и дальние зависимости, улучшая качество на задачах с длинным контекстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e76ef1ce-cb7e-43c1-bdcc-e7cb44dd9792"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
    "    The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the frequency tensor.\n",
    "        end (int): End index for precomputing frequencies.\n",
    "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n",
    "\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Reshape frequency tensor for broadcasting it with another tensor.\n",
    "\n",
    "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
    "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped frequency tensor.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
    "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f'{freqs_cis.shape} - {(x.shape[1], x.shape[-1])}'\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
    "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
    "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
    "    returned as real tensors.\n",
    "\n",
    "    Args:\n",
    "        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n",
    "        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n",
    "        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
    "    \"\"\"\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ae88caa-9141-457c-97a0-5895d30c5c75"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionRoPE(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_bias = True):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model is not divisible by h\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.query = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.key = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.value = nn.Linear(d_model, d_model, bias = qkv_bias)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.scale = self.d_head ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        query = self.query(x).view(B, L, self.n_heads, self.d_head)\n",
    "        key = self.key(x).view(B, L, self.n_heads, self.d_head)\n",
    "        value = self.value(x).view(B, L, self.n_heads, self.d_head).transpose(1,2)\n",
    "        query, key = apply_rotary_emb(query, key, precompute_freqs_cis(self.d_head, 4096)[:L].to(next(iter(self.query.parameters())).device))\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "\n",
    "        dots = (query @ key.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        mask = torch.tril(torch.ones((L, L))).to(x.device)\n",
    "        dots.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "        att_scores = dots.softmax(-1)\n",
    "        att_v = att_scores @ value\n",
    "\n",
    "        out = att_v.permute(0, 2, 1, 3).contiguous().view(B, L, D)\n",
    "\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "652eeb3f-1196-4c64-947d-58435f2a8cb4"
   },
   "outputs": [],
   "source": [
    "class FeedForwardLlama(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.w2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.w3 = nn.Linear(d_model, 4 * d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2249ae32-f63a-448c-ab9a-c16e33e1c18f"
   },
   "outputs": [],
   "source": [
    "class DecoderBlockLlama(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_bias = True):\n",
    "        super().__init__()\n",
    "        self.RMSNorm_1 = RMSNorm(d_model)\n",
    "        self.RMSNorm_2 = RMSNorm(d_model)\n",
    "\n",
    "        self.MHA = MultiHeadAttentionRoPE(d_model, n_heads, qkv_bias)\n",
    "        self.MLP = FeedForwardLlama(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.MHA(self.RMSNorm_1(x)) + x\n",
    "        x = self.MLP(self.RMSNorm_2(x)) + x\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "771ba671-d11c-46a9-94da-b89ecbefa0a9"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLlama(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_blocks, qkv_bias = True, seq_len = 2048):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.Sequential(*[DecoderBlockLlama(d_model, n_heads, qkv_bias) for _ in range(n_blocks)])\n",
    "        self.final_ln = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(next(iter(self.parameters())).device)\n",
    "        emb = self.embedding(x)\n",
    "\n",
    "        x = self.blocks(emb)\n",
    "\n",
    "        out = self.lm_head(self.final_ln(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "318ae9c4-48a4-4d89-8f46-effe1b161a8c"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "N_BLOCKS = 3\n",
    "model_llama = TransformerDecoderLlama(VOCAB_SIZE, D_MODEL, N_HEADS, N_BLOCKS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4cf2bf1-d0eb-4c52-ba9b-e7fe753473e5"
   },
   "outputs": [],
   "source": [
    "loss_fnc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_llama.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb063ebf-a606-4f5d-a3c5-69619b46f8a3"
   },
   "outputs": [],
   "source": [
    "model_llama.train()\n",
    "loss_epochs = []\n",
    "for epoch in tqdm(range(10)):\n",
    "    loss_epoch = []\n",
    "    #for input_ids, labels_ids, data_attention_mask in tqdm(train_loader):\n",
    "    #    input_ids           = input_ids .to(DEVICE)\n",
    "    #    labels_ids          = labels_ids.to(DEVICE)\n",
    "    #    data_attention_mask = data_attention_mask.to(DEVICE)\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids           = batch[\"input_ids\"     ].to(DEVICE)\n",
    "        labels_ids          = batch[\"labels\"        ].to(DEVICE)\n",
    "        data_attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "\n",
    "        preds = model_llama(input_ids)\n",
    "        preds = preds[data_attention_mask]\n",
    "        labels_ids = labels_ids[data_attention_mask]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fnc(preds, labels_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch.append(loss.item())\n",
    "\n",
    "    print(f'Loss_epoch {epoch + 1}: {np.mean(loss_epoch)}')\n",
    "    loss_epochs.append(np.mean(loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9a59a8f-66a9-4158-9db7-47f95fd8b063"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f28964f-8f47-4dc2-93eb-b7ea2ecd839e"
   },
   "outputs": [],
   "source": [
    "#input_ids, labels_ids, data_attention_mask = train_dataset[2]\n",
    "batch = train_dataset[2]\n",
    "\n",
    "input_ids           = batch[\"input_ids\"     ].to(DEVICE)\n",
    "labels_ids          = batch[\"labels\"        ].to(DEVICE)\n",
    "data_attention_mask = batch[\"attention_mask\"].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29124b46-0d06-48e0-8264-634da8d84fca"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_batch(model, symbols, is_decode = False, max_length = 20):\n",
    "\n",
    "    model.eval()\n",
    "    # if type(ids) == list:\n",
    "    #     symbols = torch.tensor(ids, device = DEVICE).view(len(ids),1)\n",
    "\n",
    "    # else:\n",
    "    #     symbols = torch.tensor(ids, device = DEVICE).view(1,1)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "\n",
    "        cur_symbols = model(symbols).argmax(-1)[:, -1][:, None]\n",
    "        symbols = torch.cat([symbols, cur_symbols], dim = 1)\n",
    "\n",
    "    if is_decode:\n",
    "        symbols = tokenizer.batch_decode(symbols, skip_special_tokens=True)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60c3297f-cb5c-4dbb-8df8-bcda26ab833e"
   },
   "outputs": [],
   "source": [
    "generate_batch(model_llama, input_ids[:2].view(1, -1).to(DEVICE), is_decode=True, max_length= 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c8fc28a-cd63-45f3-ab62-7fb8d7816356"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92c15753-bdcd-4354-ab72-219e5f0d5c31"
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "  num+= p.numel()\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dXIgBnAZSRM"
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "\n",
    "for p in model_llama.parameters():\n",
    "  num+= p.numel()\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TjBwiYvZWW-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxLvv6UaPa33"
   },
   "source": [
    "# Стратегии декодирования\n",
    "\n",
    "В последние годы растет интерес к языковой генерации благодаря появлению больших языковых моделей на основе трансформеров, обучаемых на миллионах веб-страниц, таких как знаменитая OpenAI [GPT2 model](https://openai.com/blog/better-language-models/). Результаты впечатляют, e.g. [GPT2 on unicorns](https://openai.com/blog/better-language-models/#samples), [XLNet](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e), [Controlled language with CTRL](https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/). Помимо улучшенной архитектуры transformer и большого объема обучающих данных, важную роль также сыграли улучшенные методы декодирования.\n",
    "\n",
    "В этом блокноте дается краткий обзор различных стратегий декодирования и, что более важно, показано, как * вы * можете реализовать их с минимальными усилиями, используя популярную библиотеку `transformers`!\n",
    "\n",
    "Все следующие функциональные возможности могут быть использованы для **авторегрессионной** генерации языка ([тут](http://jalammar.github.io/illustrated-gpt2/) как это делается). Если коротко, *авторегрессионная* генерация языка основана на предположении, что распределение вероятностей последовательности слов может быть разложено на произведение следующих условных вероятностей:\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "\n",
    "Авторегрессивная генерация языка теперь доступна для `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5`!\n",
    "\n",
    "Мы проведем экскурсию по наиболее известным в настоящее время методам декодирования, главным образом *Greedy search*, *Beam search*, *Top-K sampling*  и *Top-p sampling*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbzZ_IVTtoQe"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eL5Ky2ESXOzz"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue2kOQhXTAMU"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwaPRggfF_FY"
   },
   "outputs": [],
   "source": [
    "# B x seq_len x vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Y7cgu9ohXP"
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Жадный поиск просто выбирает слово с наибольшей вероятностью в качестве следующего слова: $w_t = argmax_{w}P(w | w_{1:t-1})$ на каждом временном шаге $t$. На следующем рисунке показан Greedy Search.\n",
    "\n",
    "![Greedy Search](greedy_search.png)\n",
    "\n",
    "Начиная со слова $\\text{\"The\"}$, алгоритм\n",
    "жадно выбирает следующее слово с наибольшей вероятностью $\\text{\"nice\"}$ и так далее, так что конечная сгенерированная последовательность слов будет $\\text{\"The\", \"nice\", \"woman\"}$ с общей вероятностью $0.5 \\times 0.4 = 0.2$.\n",
    "\n",
    "В дальнейшем мы будем генерировать последовательности слов, используя GPT2 в контексте $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$. Давайте посмотрим, как можно использовать жадный поиск в \"трансформерах\" следующим образом\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWLd_J6lXz_t"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBn1ePmJvhrl"
   },
   "source": [
    "Хорошо! Мы сгенерировали наш первый короткий текст с помощью GPT2 😊. Сгенерированные слова, следующие контексту, разумны, но модель быстро начинает повторяться! Это очень распространенная проблема при генерации языка в целом и, по-видимому, еще более распространена при grid / beam search'e - проверьте [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) and [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "Однако основным недостатком жадного поиска является то, что он пропускает слова с высокой вероятностью, скрытые за словом с низкой вероятностью, как видно из нашего наброска выше:\n",
    "\n",
    "Слово $\\text{\"has\"}$ с его высокой условной вероятностью $0,9$ скрыто за словом $\\text{\"dog\"}$, которое имеет только вторую по величине условную вероятность, так что жадный поиск пропускает последовательность слов $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$.\n",
    "\n",
    "К счастью, у нас есть beam search, чтобы решить эту проблему!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8DnXZ1WiuNd"
   },
   "source": [
    "### **Beam search**\n",
    "\n",
    "Поиск по лучу (beam search) снижает риск пропуска скрытых последовательностей слов с высокой вероятностью, сохраняя наиболее вероятные \"num_beams\" гипотез на каждом временном шаге и в конечном итоге выбирая гипотезу, имеющую в целом наибольшую вероятность. Давайте проиллюстрируем это с помощью `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "На временном шаге $1$, помимо наиболее вероятной гипотезы $\\text{\"The\", \"woman\"}$, beam search также отслеживает вторую наиболее вероятную гипотезу $\\text{\"The\", \"dog\"}$. На временном шаге $2$ поиск по лучу обнаруживает, что последовательность слов $\\text{\"The\", \"dog\", \"has\"}$ имеет вероятность $0,36$ более высокую, чем $\\text{\"The\", \"nice\", \"woman\"}$, которая имеет $0,2$. Отлично, он нашел наиболее вероятную последовательность слов в нашем примере!\n",
    "\n",
    "Поиск по лучу всегда найдет выходную последовательность с большей вероятностью, чем жадный поиск, но не гарантирует нахождения наиболее вероятного результата.\n",
    "\n",
    "Давайте посмотрим, как поиск по лучу можно использовать в \"трансформерах\". Мы устанавливаем `num_beams > 1` и `early_stopping=True`, чтобы генерация завершалась, когда все гипотезы луча достигали токена EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1R5kx30Ynej"
   },
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ6xs-KLi9jT"
   },
   "source": [
    "Хотя результат, возможно, более плавный, выход по-прежнему включает повторения одних и тех же последовательностей слов.  \n",
    "Простое решение состоит в том, чтобы ввести штрафы за *n-граммы* (*также известные как последовательности слов из $n$ слов), как это было введено [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304 ) и [Klein et al. (2017)](https://arxiv.org/abs/1701.02810 ). Наиболее распространенный штраф *n-граммов* гарантирует, что ни один *n-грамм* не появится дважды, вручную устанавливая вероятность следующих слов, которые могли бы создать уже встречавшийся *n-грамм*, равной $0$.\n",
    "\n",
    "Давайте попробуем это, установив `no_repeat_n_gram_size=2`, чтобы ни один биграм не появлялся дважды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy3iVJgfnkMi"
   },
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=1,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxsksOGDpmA0"
   },
   "source": [
    "Мило, это выглядит намного лучше! Мы можем видеть, что повторение больше не появляется. Тем не менее, *n-граммовые* штрафы следует использовать с осторожностью. В статье, сгенерированной о городе *Нью-Йорк*, не следует использовать штраф в размере *2-gramms* или название города будет встречаться только один раз во всем тексте!\n",
    "\n",
    "Еще одной важной особенностью поиска луча является то, что мы можем сравнить верхние лучи после генерации и выбрать сгенерированный луч, который лучше всего соответствует нашей цели.\n",
    "\n",
    "В `transformers` мы просто устанавливаем параметр `num_return_sequences` на количество лучей с наибольшим количеством баллов, которые должны быть возвращены. Убедитесь, однако, что `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ClO3VphqGp6"
   },
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZTyovJKHbWb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbbIyK84wHq6"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "В своей самой простой форме sampling означает случайный выбор следующего слова $w_t$ в соответствии с его условным распределением вероятностей:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Используя приведенный выше пример, на следующем рисунке визуализируется генерация языка при выборке.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "Становится очевидным, что генерация языка с использованием sampling больше не является \"детерминированной\". Cлово\n",
    "$\\text{\"car\"}$ выбирается из условного распределения вероятностей $P(w |\\text{\"The\"})$, за которым следует sampling $\\text{\"drives\"}$ из $P(w |\\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "В `transformers` устанавливаем `do_sample=True` и деактивируем sampling *Top-K* (подробнее об этом позже) через `top_k=0`. Не стесняйтесь изменять `random_seed`, чтобы поиграть с моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRAz4D-Ks0_4"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQHuo911wfT-"
   },
   "source": [
    "Интересно! Текст кажется нормальным, но при ближайшем рассмотрении оказывается, что он не очень связный. Слова *new hand sense* и *local batte harness* очень странные и звучат не так, как будто они были написаны человеком. Это большая проблема при выборке последовательностей слов: модели часто генерируют бессвязную тарабарщину, *ср.* [Ари Хольцман и др. (2019)](https://arxiv.org/abs/1904.09751 ).\n",
    "\n",
    "Хитрость заключается в том, чтобы сделать распределение $P(w|w_{1:t-1})$ более четким (увеличивая вероятность слов с высокой вероятностью и уменьшая вероятность слов с низкой вероятностью), понизив так называемую \"температуру\" [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max ).\n",
    "\n",
    "$q = \\frac{exp(z_i / T)}{\\sum_j exp(z_j / T)}$\n",
    "\n",
    "Иллюстрация применения температуры к нашему примеру, приведенному выше, может выглядеть следующим образом.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "Условное распределение следующего слова на шаге $t = 1$ становится намного более четким, практически не оставляя шансов для выбора слова $\\text{\"car\"}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgJredc-0j0Z"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binNTroyzQBu"
   },
   "source": [
    "## **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf ) ввел простую, но очень мощную схему семплирования, называемую выборкой ***Top-K***. В выборке *Top-K* фильтруются *K* наиболее вероятных следующих слов, и масса вероятности перераспределяется только между этими *K* следующими словами.\n",
    "GPT2 принял эту схему семплирования, что стало одной из причин его успеха.\n",
    "\n",
    "Мы расширили диапазон слов, используемых для обоих этапов семплирования в приведенном выше примере, с 3 слов до 10 слов, чтобы лучше проиллюстрировать выборку *Top-K*.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Установив $K = 6$, на обоих этапах семплирования мы ограничиваем наш пул выборок 6 словами. В то время как 6 наиболее вероятных слов, определенных как $V_ {\\text{top-K}}$, охватывают только *около* двух третей всей массы вероятности на первом шаге, они включают почти всю массу вероятности на втором шаге. Тем не менее, мы видим, что он успешно устраняет довольно странных кандидатов $\\text{\"not\", \"the\", \"small\", \"told\"}$\n",
    "на втором этапе отбора проб.\n",
    "\n",
    "\n",
    "Давайте посмотрим, как *Top-K* можно использовать в библиотеке, установив `top_k=50`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBtDOdD0wx3l"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77H5m4ZmhEX"
   },
   "source": [
    "Совсем неплохо! Этот текст, пожалуй, самый \"человечный\" на сегодняшний день.\n",
    "Однако одна проблема с выборкой *Top-K* заключается в том, что она динамически не адаптирует количество слов, которые отфильтровываются из распределения вероятности следующего слова $P (w |w_ {1:t-1})$.\n",
    "Это может быть проблематично, поскольку некоторые слова могут быть выбраны из очень четкого распределения (распределение справа на графике выше), в то время как другие - из гораздо более плоского распределения (распределение слева на графике выше).\n",
    "\n",
    "На шаге $t=1$, *Top-K* исключает возможность\n",
    "примеров $\\text{\"people\", \"big\", \"house\", \"cat\"}$, которые кажутся разумными кандидатами. С другой стороны, на шаге $t= 2$ метод включает, возможно, неправильно подобранные слова $\\text{\"down\", \"a\"}$ в выборку слов. Таким образом, ограничение пула выборок фиксированным размером *K* может привести к тому, что модель будет выдавать тарабарщину для четких распределений и ограничит творческий потенциал модели для плоского распределения.\n",
    "Эта интуиция привела [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751 ) для создания ***Top-p*** - или ***nucleus***-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9LAaexzV3H"
   },
   "source": [
    "## **Sampling Top-p (nucleus)**\n",
    "\n",
    "Вместо выборки только из наиболее вероятных *K* слов, в выборке *Top-p* выбирается из наименьшего возможного набора слов, совокупная вероятность которых превышает вероятность *p*. Затем масса вероятности перераспределяется между этим набором слов. Таким образом, размер набора слов (количество слов в наборе) может динамически увеличиваться и уменьшаться в соответствии с распределением вероятности следующего слова. Ладно, это было очень многословно, давайте визуализируем.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Установив $p=0.92$, выборка *Top-p* выбирает *минимальное* количество слов, которое в совокупности должно превышать $p=92\\%$ от массы вероятности, определяемой как $V_{\\text{top-p}}$. В первом примере это включало 9 наиболее вероятных слов, тогда как во втором примере нужно выбрать только 3 лучших слова, чтобы их вероятностная масса превысила 92%. На самом деле все очень просто! Можно видеть, что стратегия сохраняет широкий диапазон слов, где следующее слово, возможно, менее предсказуемо, *например* $P(w | \\text{\"The\"})$, и только несколько слов, когда следующее слово кажется более предсказуемым, *например* $P(w | \\text{\"The\", \"car\"})$.\n",
    "\n",
    "Мы активируем выборку *Top-p*, установив `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvwIc7YAx77F"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(3)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_p=0.8,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-8gLaR4lat"
   },
   "source": [
    "Отлично, звучит так, словно это мог бы написать человек. Ну, может быть, еще не совсем.\n",
    "\n",
    "Хотя в теории *Top-p* кажется более элегантным, чем *Top-K*, оба метода хорошо работают на практике. *Top-p* также можно использовать в сочетании с *Top-K*, что позволяет избежать слов с очень низким скором, обеспечивая при этом некоторый динамический выбор.\n",
    "\n",
    "Наконец, чтобы получить несколько независимо отобранных выходных данных, мы можем установить параметр `num_return_sequences > 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kY8P9VG8Gi9"
   },
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZpN3QTweA0J"
   },
   "source": [
    "## Constrained generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYsyCeBUJAVy"
   },
   "source": [
    "Для лучшей читаемости мы будем использовать end-of-line в качестве токена EOS, вместо того, чтобы просто всегда генерировать 50 токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUK1B7WsJCpM"
   },
   "outputs": [],
   "source": [
    "END_OF_LINE = tokenizer('\\n').input_ids[0]\n",
    "print(END_OF_LINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBATNPrxJwPB"
   },
   "source": [
    "Поиск по базовому лучу выдает очень похожие предложения; все они содержат \"not sure\" или \"don't think\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgsoFsmOI9Pe"
   },
   "outputs": [],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj3S7FkgI9lv"
   },
   "source": [
    "Что произойдет, если мы запретим модели использовать эти фразы?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-nlHOYzJ_bP"
   },
   "outputs": [],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think', 'thundersnatch'], add_prefix_space=True)['input_ids'],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng2xaRxaMSXS"
   },
   "outputs": [],
   "source": [
    "tokenizer(['sure', ' sure', ' I am not sure'])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J16fr3qDKOz3"
   },
   "source": [
    "Мы видим, что смысл этих текстов сильно изменился – но каким-то непредсказуемым образом.\n",
    "\n",
    "Можем ли мы заставить модель написать текст с участием \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsOq5q5PKspB"
   },
   "outputs": [],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think'], add_prefix_space=True)['input_ids'],\n",
    "    force_words_ids=[tokenizer(['cat'], add_prefix_space=True, add_special_tokens=False).input_ids],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40lA3WrhK726"
   },
   "source": [
    "Уточнение: **force_words_ids** - это список ограничений. Каждое ограничение представляет собой список выражений, таких, что по крайней мере одно выражение должно быть включено в сгенерированный текст. И каждое выражение - это просто список токенов.\n",
    "\n",
    "Смотрите обсуждение в [the HF pull request](https://github.com/huggingface/transformers/issues/14081 ), или прочтите статью \"[Guided Generation of Cause and Effect](https://www.ijcai.org/proceedings/2020/0502.pdf )\" Ли и др., где был предложен алгоритм.  \n",
    "\n",
    "Чтобы оценить силу этих ограничений, давайте заставим модель включить мышь (или даже много мышей) в текст. Мы также можем ослабить ограничение \"кошка\", разрешив вместо него использовать слова \"кошки\", \"котенок\" или \"кошачий\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXvaYLFFLQWT"
   },
   "outputs": [],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=END_OF_LINE,\n",
    "    bad_words_ids=tokenizer(['sure', 'think'], add_prefix_space=True)['input_ids'],\n",
    "    force_words_ids = [\n",
    "        tokenizer(['cat', 'cats', 'kitten', 'feline', 'Cat', 'Cats'], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "        tokenizer(['mouse', 'mice'], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWd7e98Vcs3"
   },
   "source": [
    "## **Заключение**\n",
    "\n",
    "Методы декодирования *ad-hoc*, выборка *top-p* и *top-K*, по-видимому, обеспечивают более плавный текст, чем традиционный *жадный* и *лучевой* поиск при генерации открытого языка.\n",
    "Однако в последнее время появилось больше доказательств того, что очевидные недостатки *жадного* и *лучевого* поиска - в основном генерирующие повторяющиеся последовательности слов - вызваны моделью (особенно способом обучения модели), а не методом декодирования,  [Уэллек и др. (2019)](https://arxiv.org/pdf/1908.04319.pdf ). Кроме того, как показано в [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492 ), похоже, что выборки *top-K* и *top-p* также страдают от генерации повторяющихся последовательностей слов.\n",
    "\n",
    "В [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf ), авторы показывают, что, согласно человеческим оценкам, лучевой поиск может генерировать более плавный текст, чем выборка *Top-p*, при адаптации цели обучения модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4CYi91h11yd"
   },
   "source": [
    "## **Приложение**\n",
    "\n",
    "Есть пара дополнительных параметров для метода `generate`, которые не были упомянуты выше. Мы кратко объясним их здесь!\n",
    "\n",
    "- `min_length` можно использовать, чтобы заставить модель не выдавать токен EOS (= не заканчивать предложение) до достижения `min_length`. Это довольно часто используется при суммаризации, но может быть полезно в целом, если пользователь хочет получить более длинные выходные данные.\n",
    "- `repetition_penalty` может использоваться для \"наказания\" слов, которые уже были сгенерированы или принадлежат контексту. Впервые он был представлен [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858 ) и также используется в цели обучения в [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf ). Это может быть довольно эффективно для предотвращения повторений, но, по-видимому, очень чувствительно к различным моделям и вариантам использования, *например* смотрите это [обсуждение](https://github.com/huggingface/transformers/pull/2303 ) на Github.\n",
    "\n",
    "- `attention_mask` можно использовать для маскировки дополненных токенов\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: Если в модели по умолчанию нет этих токенов, пользователь может вручную выбрать другие идентификаторы токенов для их представления.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxjHF8KfqrKi"
   },
   "outputs": [],
   "source": [
    "!pip install vllm lm-format-enforcer triton -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1YuGEpLUiey"
   },
   "source": [
    "We load the model, as is normally done with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTkxF4fNbfYE"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBOcaL7aUiey"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2-mYt_YUiey"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_header(text):\n",
    "    display(Markdown(f'**{text}**'))\n",
    "\n",
    "def display_content(text):\n",
    "    display(Markdown(f'```\\n{text}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXrsjFZyUiey"
   },
   "source": [
    "## Скомбинируем CharacterLevelParser с vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03zPW3XmUiey"
   },
   "outputs": [],
   "source": [
    "from lmformatenforcer import CharacterLevelParser\n",
    "from lmformatenforcer.integrations.vllm import build_vllm_logits_processor, build_vllm_token_enforcer_tokenizer_data\n",
    "from typing import Union, List, Optional\n",
    "from vllm import SamplingParams\n",
    "\n",
    "DEFAULT_MAX_NEW_TOKENS = 100\n",
    "\n",
    "ListOrStrList = Union[str, List[str]]\n",
    "\n",
    "tokenizer_data = build_vllm_token_enforcer_tokenizer_data(llm)\n",
    "\n",
    "def vllm_with_character_level_parser(prompt: ListOrStrList, parser: Optional[CharacterLevelParser] = None) -> ListOrStrList:\n",
    "    sampling_params = SamplingParams()\n",
    "    sampling_params.max_tokens = DEFAULT_MAX_NEW_TOKENS\n",
    "    if parser:\n",
    "        logits_processor = build_vllm_logits_processor(tokenizer_data, parser)\n",
    "        sampling_params.logits_processors = [logits_processor]\n",
    "    results = llm.generate(prompt, sampling_params=sampling_params)\n",
    "    if isinstance(prompt, str):\n",
    "        return results[0].outputs[0].text\n",
    "    else:\n",
    "        return [result.outputs[0].text for result in results]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9oecpIkUiey"
   },
   "source": [
    "## vLLM + JSON Use case\n",
    "\n",
    "Теперь мы продемонстрируем использование ``JsonSchemaParser``. Мы создаем Pydantic model, генерируем на ее основе схему и используем ее для costrained decoding.\n",
    "Выходные данные всегда будут иметь формат, который может быть разобран парсером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOe1I9-NUiey"
   },
   "outputs": [],
   "source": [
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.model_json_schema()}'\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.92, max_tokens=100)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": question_with_schema},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompts)\n",
    "\n",
    "display_header(\"Answer, With json schema enforcing:\")\n",
    "\n",
    "result = vllm_with_character_level_parser(prompts, JsonSchemaParser(AnswerFormat.model_json_schema()))\n",
    "display_content(result)\n",
    "\n",
    "display_header(\"Answer, Without json schema enforcing:\")\n",
    "result = vllm_with_character_level_parser(prompts, None)\n",
    "display_content(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OLAFhhZUiez"
   },
   "source": [
    "As you can see, the enforced output matches the required schema, while the unenforced does not. We have successfully integrated with vLLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv-Xl1laUiez"
   },
   "source": [
    "## Batching example\n",
    "\n",
    "Now we demonstrate that the model can be used to generate text in batches. This is useful for generating text in parallel, which is much faster than generating text sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NohtPQ_kUiez"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "players = ['Michael Jordan', 'Tim Duncan', 'Larry Bird', 'Magic Johnson', 'Patrick Ewing',\n",
    "           'Hakeem Olajuwan', 'Nate Archibald', 'Charles Barkley', 'Bob Cousy', 'Clyde Drexler',\n",
    "           'Julius Erving', 'John Havlicek', 'Elvin Hayes', 'Jerry Lucas', 'Moses Malone',\n",
    "           'George Mikan', 'Bob Pettit', 'Oscar Robertson', 'Bill Russell', 'Dolph Schayes']\n",
    "prompts = []\n",
    "for player in players:\n",
    "    question = f'Please give me information about {player}. You MUST answer using the following json schema: '\n",
    "    question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question_with_schema},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "start = time()\n",
    "one_player_result = vllm_with_character_level_parser(prompts[0], JsonSchemaParser(AnswerFormat.schema()))\n",
    "end = time()\n",
    "print(f'Time taken for 1 player: {end - start}s')\n",
    "display_content(one_player_result)\n",
    "\n",
    "start = time()\n",
    "all_results = vllm_with_character_level_parser(prompts[1:], JsonSchemaParser(AnswerFormat.schema()))\n",
    "end = time()\n",
    "print(f'Time taken for {len(prompts)-1} players: {end - start}. Time per player: {(end - start)/(len(prompts)-1)}')\n",
    "display_content(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYKTw9_bcn06"
   },
   "outputs": [],
   "source": [
    "print(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejo0YNPeUiez"
   },
   "source": [
    "# Регулярное выражение + пример анализа\n",
    "\n",
    "Теперь мы покажем две дополнительные возможности: Поддержка регулярных выражений и анализ помех.\n",
    "\n",
    "Код здесь более низкоуровневый, так как нам нужен экземпляр `logits_processor`, чтобы не вызывать вспомогательную функцию `vllm_with_character_level_parser`, которую мы создали ранее в этом блокноте.\n",
    "\n",
    "Анализ вмешательства позволяет нам увидеть, насколько сильно пришлось действовать форматировщику и какова была бы вероятность появления выбранных лексем, если бы форматировщик не вмешивался. Это может помочь вам улучшить качество результатов, улучшив подсказки и моделирование, чтобы уменьшить редактирование декодируемой последовательности. Как правило, чем меньше вмешательства, тем лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GETPhXPIUiez"
   },
   "outputs": [],
   "source": [
    "from lmformatenforcer.regexparser import RegexParser\n",
    "import pandas as pd\n",
    "\n",
    "date_regex = r'(0?[1-9]|1[0-2])\\/(0?[1-9]|1\\d|2\\d|3[01])\\/(19|20)\\d{2}'\n",
    "answer_regex = ' In mm/dd/yyyy format, Michael Jordan was born in ' + date_regex\n",
    "parser = RegexParser(answer_regex)\n",
    "\n",
    "question = 'When was Michael Jordan Born? Please answer in mm/dd/yyyy format.'\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "display_header(\"Prompt:\")\n",
    "display_content(prompt)\n",
    "\n",
    "# Note the analyze=True flag, which is will create an analyzer in the processor\n",
    "logits_processor = build_vllm_logits_processor(tokenizer_data, parser, analyze=True)\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=200, logits_processors=[logits_processor])\n",
    "results = llm.generate(prompt, sampling_params=sampling_params)\n",
    "\n",
    "text = results[0].outputs[0].text\n",
    "display_header(\"Answer:\")\n",
    "display_content(text)\n",
    "\n",
    "display_header(\"Analyzer Results:\")\n",
    "report_dict = logits_processor.analyzer.generate_report_dict(results[0].outputs[0].token_ids)\n",
    "enforced_scores = pd.DataFrame(report_dict)\n",
    "# Setting some display options for readability\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.float_format', ' {:,.5f}'.format)\n",
    "display(enforced_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouGaMW6mUiez"
   },
   "source": [
    "The timesteps in which `generated_score < leading_score` are those in which the format enforcer had to intervene. Consider using this during development to fine tune your prompts for better consistency.\n",
    "\n",
    "This method also works for JSON Schema mode, of course."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
