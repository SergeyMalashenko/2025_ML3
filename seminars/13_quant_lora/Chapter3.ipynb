{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Глава 3: Низкоранговая адаптация (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Содержание главы\n",
    "\n",
    " В этой главе мы:\n",
    "\n",
    "\n",
    "\n",
    " *   Поймем, что такое низкоранговый адаптер и почему он полезен.\n",
    "\n",
    " *   Подготовим квантованную модель для обучения.\n",
    "\n",
    " *   Используем библиотеку `peft` для создания и подключения адаптеров к базовой модели.\n",
    "\n",
    " *   Обсудим параметры конфигурации для выбора целевых слоев при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (0.48.2)\n",
      "Requirement already satisfied: trl in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.1)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from trl) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: psutil in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (7.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.21.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Если вы используете Colab\n",
    "!pip install datasets bitsandbytes trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вы используете Jupyter Template от runpod.io\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import matrix_rank\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Цель использования адаптеров\n",
    "\n",
    " Мы подключаем адаптеры к огромным линейным слоям в LLM, чтобы резко сократить количество обучаемых параметров. Мы можем легко уменьшить количество обучаемых параметров до менее чем 1% от их исходного числа. За счет сокращения как вычислений (меньше градиентов для расчета), так и объема памяти (меньше параметров, отслеживаемых оптимизатором), мы достигаем значительного прироста эффективности. Однако следует помнить, что низкоранговые адаптеры вряд ли сравнятся по производительности с полной настройкой модели, и их эффективность может варьироваться в зависимости от базовой модели и задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Предварительные сведения\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n",
    "\n",
    " <center>Рисунок 3.1 - Умножение матриц</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Суть низкоранговой адаптации\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n",
    "\n",
    " <center>Рисунок 3.2 - Умножение двух низкоранговых матриц</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer = nn.Linear(1024, 1024, bias=False)\n",
    "base_layer.weight.shape, base_layer.weight.numel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n",
    "\n",
    " <center>Рисунок 3.3 - Замороженные веса и низкоранговые матрицы</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=1024, out_features=8, bias=False),\n",
       " Linear(in_features=8, out_features=1024, bias=False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "r = 8\n",
    "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "layer_A, layer_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 8192)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_A.weight.numel(), layer_B.weight.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite = layer_B.weight @ layer_A.weight\n",
    "composite.shape, composite.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_rank(composite.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{output} = X @ (W + B @ A)^T\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 3.1 - Добавление результирующего произведения к весам</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3909,  1.1422,  0.0897,  ..., -0.5231,  0.1517,  0.6949]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "batch = torch.randn(1, 1024)\n",
    "\n",
    "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 3.2 - Использование двух прямых проходов</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n",
    "\n",
    " <center>Рисунок 3.4 - Использование двух прямых проходов</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3552,  1.1192,  0.5504,  ..., -0.2310, -0.0427,  0.1908]]),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = batch @ base_layer.weight.data.T\n",
    "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
    "regular_output, additional_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 3.3 - Цепочка прямых проходов адаптера</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_A = (batch @ layer_A.weight.T)\n",
    "additional_output = out_A @ layer_B.weight.T\n",
    "additional_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3552,  1.1192,  0.5504,  ..., -0.2310, -0.0427,  0.1908]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.3909,  1.1422,  0.0897,  ..., -0.5231,  0.1517,  0.6949]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = base_layer(batch)\n",
    "out_A = layer_A(batch)\n",
    "additional_output = layer_B(out_A)\n",
    "output = regular_output + additional_output\n",
    "regular_output, additional_output, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 3.4 - Параметр alpha в LoRA</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4267,  1.1652, -0.3710,  ..., -0.8151,  0.3461,  1.1989]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 2*r\n",
    "output = regular_output + (alpha / r) * additional_output\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Итог предыдущих глав"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaa9a134836498d96816b378b03475a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download entire repository\n",
    "snapshot_download(\n",
    "    repo_id=\"facebook/opt-350m\",\n",
    "    local_dir=\"./opt-350m-model\",\n",
    "    ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"],  # Skip unnecessary formats\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# Load from local directory\n",
    "model_dir = \"./opt-350m-model\"\n",
    "\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Типы параметров и градиенты\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"Типы параметров и градиенты\"**\n",
    "\n",
    " *   Квантование замораживает только те линейные слои, которые были квантованы.\n",
    "\n",
    " *   После квантования модель можно подготовить с помощью функции `prepare_model_for_kbit_training()`.\n",
    "\n",
    "     *   Она замораживает **все** слои.\n",
    "\n",
    "     *   Она преобразует каждый неквантованный 16-битный слой в FP32 для улучшения обучения.\n",
    "\n",
    "     *   Она включает контрольные точки градиента (gradient checkpointing).\n",
    "\n",
    " *   Позже вы сможете разморозить выбранные слои с помощью конфигурации LoRA.\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decoder.embed_tokens.weight', torch.bfloat16),\n",
       " ('decoder.embed_positions.weight', torch.bfloat16),\n",
       " ('decoder.layers.0.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.0.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.0.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.0.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.1.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.1.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.1.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.1.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.2.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.2.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.2.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.2.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.3.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.3.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.3.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.3.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.4.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.4.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.4.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.4.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.5.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.5.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.5.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.5.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.6.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.6.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.6.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.6.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.7.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.7.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.7.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.7.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.8.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.8.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.8.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.8.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.9.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.9.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.9.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.9.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.10.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.10.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.10.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.10.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.11.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.11.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.11.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.11.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.12.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.12.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.12.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.12.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.13.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.13.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.13.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.13.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.14.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.14.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.14.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.14.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.15.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.15.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.15.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.15.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.16.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.16.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.16.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.16.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.17.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.17.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.17.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.17.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.18.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.18.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.18.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.18.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.19.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.19.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.19.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.19.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.20.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.20.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.20.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.20.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.21.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.21.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.21.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.21.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.22.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.22.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.22.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.22.final_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.23.self_attn_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.23.self_attn_layer_norm.bias', torch.bfloat16),\n",
       " ('decoder.layers.23.final_layer_norm.weight', torch.bfloat16),\n",
       " ('decoder.layers.23.final_layer_norm.bias', torch.bfloat16)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainable_parms(model):\n",
    "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
    "    return parms\n",
    "\n",
    "trainable_parms(model_q4.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Функция `prepare_model_for_kbit_training()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "                                        use_gradient_checkpointing=True,\n",
    "                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "prepared_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(prepared_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parms_of_dtype(model, dtype=torch.float32):\n",
    "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
    "    return parms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.decoder.embed_tokens.weight',\n",
       " 'model.decoder.embed_positions.weight',\n",
       " 'model.decoder.layers.0.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.0.fc1.bias',\n",
       " 'model.decoder.layers.0.fc2.bias',\n",
       " 'model.decoder.layers.0.final_layer_norm.weight',\n",
       " 'model.decoder.layers.0.final_layer_norm.bias',\n",
       " 'model.decoder.layers.1.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.1.fc1.bias',\n",
       " 'model.decoder.layers.1.fc2.bias',\n",
       " 'model.decoder.layers.1.final_layer_norm.weight',\n",
       " 'model.decoder.layers.1.final_layer_norm.bias',\n",
       " 'model.decoder.layers.2.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.2.fc1.bias',\n",
       " 'model.decoder.layers.2.fc2.bias',\n",
       " 'model.decoder.layers.2.final_layer_norm.weight',\n",
       " 'model.decoder.layers.2.final_layer_norm.bias',\n",
       " 'model.decoder.layers.3.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.3.fc1.bias',\n",
       " 'model.decoder.layers.3.fc2.bias',\n",
       " 'model.decoder.layers.3.final_layer_norm.weight',\n",
       " 'model.decoder.layers.3.final_layer_norm.bias',\n",
       " 'model.decoder.layers.4.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.4.fc1.bias',\n",
       " 'model.decoder.layers.4.fc2.bias',\n",
       " 'model.decoder.layers.4.final_layer_norm.weight',\n",
       " 'model.decoder.layers.4.final_layer_norm.bias',\n",
       " 'model.decoder.layers.5.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.5.fc1.bias',\n",
       " 'model.decoder.layers.5.fc2.bias',\n",
       " 'model.decoder.layers.5.final_layer_norm.weight',\n",
       " 'model.decoder.layers.5.final_layer_norm.bias',\n",
       " 'model.decoder.layers.6.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.6.fc1.bias',\n",
       " 'model.decoder.layers.6.fc2.bias',\n",
       " 'model.decoder.layers.6.final_layer_norm.weight',\n",
       " 'model.decoder.layers.6.final_layer_norm.bias',\n",
       " 'model.decoder.layers.7.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.7.fc1.bias',\n",
       " 'model.decoder.layers.7.fc2.bias',\n",
       " 'model.decoder.layers.7.final_layer_norm.weight',\n",
       " 'model.decoder.layers.7.final_layer_norm.bias',\n",
       " 'model.decoder.layers.8.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.8.fc1.bias',\n",
       " 'model.decoder.layers.8.fc2.bias',\n",
       " 'model.decoder.layers.8.final_layer_norm.weight',\n",
       " 'model.decoder.layers.8.final_layer_norm.bias',\n",
       " 'model.decoder.layers.9.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.9.fc1.bias',\n",
       " 'model.decoder.layers.9.fc2.bias',\n",
       " 'model.decoder.layers.9.final_layer_norm.weight',\n",
       " 'model.decoder.layers.9.final_layer_norm.bias',\n",
       " 'model.decoder.layers.10.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.10.fc1.bias',\n",
       " 'model.decoder.layers.10.fc2.bias',\n",
       " 'model.decoder.layers.10.final_layer_norm.weight',\n",
       " 'model.decoder.layers.10.final_layer_norm.bias',\n",
       " 'model.decoder.layers.11.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.11.fc1.bias',\n",
       " 'model.decoder.layers.11.fc2.bias',\n",
       " 'model.decoder.layers.11.final_layer_norm.weight',\n",
       " 'model.decoder.layers.11.final_layer_norm.bias',\n",
       " 'model.decoder.layers.12.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.12.fc1.bias',\n",
       " 'model.decoder.layers.12.fc2.bias',\n",
       " 'model.decoder.layers.12.final_layer_norm.weight',\n",
       " 'model.decoder.layers.12.final_layer_norm.bias',\n",
       " 'model.decoder.layers.13.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.13.fc1.bias',\n",
       " 'model.decoder.layers.13.fc2.bias',\n",
       " 'model.decoder.layers.13.final_layer_norm.weight',\n",
       " 'model.decoder.layers.13.final_layer_norm.bias',\n",
       " 'model.decoder.layers.14.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.14.fc1.bias',\n",
       " 'model.decoder.layers.14.fc2.bias',\n",
       " 'model.decoder.layers.14.final_layer_norm.weight',\n",
       " 'model.decoder.layers.14.final_layer_norm.bias',\n",
       " 'model.decoder.layers.15.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.15.fc1.bias',\n",
       " 'model.decoder.layers.15.fc2.bias',\n",
       " 'model.decoder.layers.15.final_layer_norm.weight',\n",
       " 'model.decoder.layers.15.final_layer_norm.bias',\n",
       " 'model.decoder.layers.16.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.16.fc1.bias',\n",
       " 'model.decoder.layers.16.fc2.bias',\n",
       " 'model.decoder.layers.16.final_layer_norm.weight',\n",
       " 'model.decoder.layers.16.final_layer_norm.bias',\n",
       " 'model.decoder.layers.17.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.17.fc1.bias',\n",
       " 'model.decoder.layers.17.fc2.bias',\n",
       " 'model.decoder.layers.17.final_layer_norm.weight',\n",
       " 'model.decoder.layers.17.final_layer_norm.bias',\n",
       " 'model.decoder.layers.18.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.18.fc1.bias',\n",
       " 'model.decoder.layers.18.fc2.bias',\n",
       " 'model.decoder.layers.18.final_layer_norm.weight',\n",
       " 'model.decoder.layers.18.final_layer_norm.bias',\n",
       " 'model.decoder.layers.19.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.19.fc1.bias',\n",
       " 'model.decoder.layers.19.fc2.bias',\n",
       " 'model.decoder.layers.19.final_layer_norm.weight',\n",
       " 'model.decoder.layers.19.final_layer_norm.bias',\n",
       " 'model.decoder.layers.20.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.20.fc1.bias',\n",
       " 'model.decoder.layers.20.fc2.bias',\n",
       " 'model.decoder.layers.20.final_layer_norm.weight',\n",
       " 'model.decoder.layers.20.final_layer_norm.bias',\n",
       " 'model.decoder.layers.21.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.21.fc1.bias',\n",
       " 'model.decoder.layers.21.fc2.bias',\n",
       " 'model.decoder.layers.21.final_layer_norm.weight',\n",
       " 'model.decoder.layers.21.final_layer_norm.bias',\n",
       " 'model.decoder.layers.22.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.22.fc1.bias',\n",
       " 'model.decoder.layers.22.fc2.bias',\n",
       " 'model.decoder.layers.22.final_layer_norm.weight',\n",
       " 'model.decoder.layers.22.final_layer_norm.bias',\n",
       " 'model.decoder.layers.23.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.23.fc1.bias',\n",
       " 'model.decoder.layers.23.fc2.bias',\n",
       " 'model.decoder.layers.23.final_layer_norm.weight',\n",
       " 'model.decoder.layers.23.final_layer_norm.bias']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms_of_dtype(prepared_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264.15104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model.get_memory_footprint()/1e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Библиотека PEFT\n",
    "\n",
    " \"*🤗 PEFT (Parameter-Efficient Fine-Tuning) — это библиотека для эффективной адаптации больших предварительно обученных моделей к различным прикладным задачам без тонкой настройки всех параметров модели, поскольку это непомерно дорого. Методы PEFT настраивают только небольшое количество (дополнительных) параметров модели, значительно снижая вычислительные затраты и затраты на хранение, при этом обеспечивая производительность, сравнимую с полностью настроенной моделью. Это делает обучение и хранение больших языковых моделей (LLM) на потребительском оборудовании более доступным.*\n",
    "\n",
    "\n",
    "\n",
    " *PEFT интегрирована с библиотеками Transformers, Diffusers и Accelerate, чтобы обеспечить более быстрый и простой способ загрузки, обучения и использования больших моделей для вывода.*\"\n",
    "\n",
    "\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"PEFT\"**\n",
    "\n",
    " *   Базовая конфигурация ниже должна хорошо работать во многих случаях.\n",
    "\n",
    " ```python\n",
    "\n",
    " config = LoraConfig(\n",
    "\n",
    "     r=16,\n",
    "\n",
    "     lora_alpha=32,\n",
    "\n",
    "     lora_dropout=0.05,\n",
    "\n",
    "     bias=\"none\",\n",
    "\n",
    "     task_type=\"CAUSAL_LM\",\n",
    "\n",
    " )\n",
    "\n",
    " peft_model = get_peft_model(model, config)\n",
    "\n",
    " ```\n",
    "\n",
    " *   Ранги 8, 16 или 32 являются типичными, но использование более высоких значений не должно значительно влиять на объем памяти модели.\n",
    "\n",
    " *   Масштабирующий коэффициент `lora_alpha` обычно в два раза больше ранга.\n",
    "\n",
    " *   Если ваша модель имеет слои `Conv1D`, добавьте `fan_in_fan_out=True` в конфигурацию.\n",
    "\n",
    " *   Если ваша модель была выпущена недавно, вам может потребоваться вручную указать `target_modules`.\n",
    "\n",
    "     *   Обычно используются имена массивных линейных слоев в модуле внимания.\n",
    "\n",
    " *   По умолчанию обучаемыми являются только адаптеры.\n",
    "\n",
    "     *   Если вы хотите обучать другие слои, такие как слои нормализации, добавьте их в аргумент `modules_to_save`.\n",
    "\n",
    "     *   Если вы добавляете собственные токены в токенизатор, вам также нужно будет обучить слои, связанные со словарем, такие как эмбеддинги и голову модели.\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig()\n",
    "lora_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Параметр `target_modules`\n",
    "\n",
    " Поскольку новые модели и архитектуры выпускаются еженедельно, вполне вероятно, что в установленной версии библиотеки PEFT нет предварительно настроенного списка целевых слоев. В этом случае вы столкнетесь со следующей ошибкой:\n",
    "\n",
    "\n",
    "\n",
    " ***\n",
    "\n",
    " `ValueError: Please specify `target_modules` in `peft_config``\n",
    "\n",
    " ***\n",
    "\n",
    "\n",
    "\n",
    " Как только вы получите имена, вы можете использовать еще один аргумент конфигурации: `target_modules`, который представляет собой имя или список имен модулей, к которым вы хотите применить адаптеры.\n",
    "\n",
    "\n",
    "\n",
    " **Поддерживаемые модели**\n",
    "\n",
    "\n",
    "\n",
    " Если вы хотите проверить, поддерживается ли архитектура данной модели установленной версией пакета `peft`, вы можете сделать следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'llama4', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'gemma2', 'gemma3_text', 'qwen2', 'qwen3'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj', 'fc1', 'fc2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### PEFT-модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
    "peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Linear4bit(\n",
       "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
    "lin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(peft_model.base_model.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Параметр `modules_to_save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поскольку модель изменяется на месте, нам необходимо выгрузить адаптеры\n",
    "# из предыдущей конфигурации, чтобы избежать их смешивания.\n",
    "# В обычном рабочем процессе вы загружаете конфигурацию только один раз,\n",
    "# и это не потребуется.\n",
    "_ = peft_model.unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 332,081,152 || trainable%: 0.2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm', 'embed_tokens']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поскольку модель изменяется на месте, нам необходимо выгрузить адаптеры\n",
    "# из предыдущей конфигурации, чтобы избежать их смешивания.\n",
    "# В обычном рабочем процессе вы загружаете конфигурацию только один раз,\n",
    "# и это не потребуется.\n",
    "_ = peft_model.unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26,624,000 || all params: 357,820,416 || trainable%: 7.4406\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поскольку модель изменяется на месте, нам необходимо выгрузить адаптеры\n",
    "# из предыдущей конфигурации, чтобы избежать их смешивания.\n",
    "# В обычном рабочем процессе вы загружаете конфигурацию только один раз,\n",
    "# и это не потребуется.\n",
    "_ = peft_model.unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,192,704 || all params: 358,128,384 || trainable%: 0.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Embedding(\n",
       "  (base_layer): Embedding(50272, 512, padding_idx=1)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict()\n",
       "  (lora_B): ModuleDict()\n",
       "  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50272 (cuda:0)])\n",
       "  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x8 (cuda:0)])\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
    "lin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Управление адаптерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_dir = \"./opt-350m-lora-yoda\"\n",
    "peft_model.load_adapter(adapter_dir, adapter_name='yoda')\n",
    "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
    "lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (third): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
    "lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.delete_adapter(adapter_name='third')\n",
    "lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'yoda'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.peft_config.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.active_adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yoda'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.set_adapter('yoda')\n",
    "peft_model.active_adapter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```python\n",
    "\n",
    " with peft_model.disable_adapter():\n",
    "\n",
    "     original_outputs = peft_model(inputs)\n",
    "\n",
    "\n",
    "\n",
    " original_outputs = peft_model.base_model(inputs)\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:424: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens', 'model.decoder.embed_tokens'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n",
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.merge_adapter(adapter_names=['yoda'])\n",
    "lora_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTAttention(\n",
       "  (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.unload()\n",
    "peft_model.base_model.model.model.decoder.layers[0].self_attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Что ждет в книге \"Fine-Tuning LLMs\"\n",
    "\n",
    " Низкоранговые адаптеры спасли положение, позволив быструю и дешевую тонкую настройку LLM. Эти огромные модели, хотя и мощные, являются мастерами одного дела — предсказания следующего токена — и поэтому остаются ограниченными структурой своих входных данных. Чтобы эти существа могли общаться, необходимо разработать новый вид ввода. Узнайте больше об удивительной истории шаблонов чата в следующей главе \"Fine-Tuning LLMs\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
