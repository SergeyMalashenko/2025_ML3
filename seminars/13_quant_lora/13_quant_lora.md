---
marp: true
class:
  - lead
paginate: true
header: 'Семинар: Упрощение нейронных сетей | Квантизация, LoRA и дообучение LLM'
footer: 'По материалам книги Даниэля Годоя «A Hands-On Guide to Fine-Tuning LLMs»'
math: mathjax
style:
---

# **Практическая настройка больших языковых моделей: от теории к развертыванию**  
*Семинар для магистрантов*  
Преподаватель: Лощилов А.А.
12 декабря 2025

---

### **2. План лекции**
1.  **Введение и мотивация**  
    *Почему тонкая настройка LLM — это вызов для одного GPU?*
2.  **Квантование**  
    *Как сжать модель в 8 раз без катастрофической потери качества?*
3.  **Параметрически-эффективная настройка (PEFT/LoRA)**  
    *Как адаптировать 3% параметров для новой задачи?*
4.  **Форматирование данных и развертывание**  
    *Как "разговаривать" с моделью и запустить её локально?*
5.  **Заключение и дальнейшие шаги**

---

### **3. Ключевой вопрос доклада**
_**«Как адаптировать LLM, когда $(N_{params} \cdot 4 \text{ bytes}) > \text{VRAM\_GPU}$?»**_

*Пример:*
*   Модель **7B** параметров в **FP32**: $7 \cdot 10^9 \times 4 \text{ bytes} \approx 28 \text{ GB VRAM}$ 
*   Типичный GPU исследователя (**NVIDIA RTX 4090**): $24 \text{ GB VRAM}$
*   **Проблема:** Даже для инференса памяти не хватает, не говоря о тонкой настройке.

**➡️ Нужны методы, позволяющие работать с LLM на потребительском железе.**

---

## **Часть 1: Введение и мотивация**

### **4. Контекст: разрыв между индустрией и академией**
*   **Индустрия (Big Tech):** Кластеры из тысяч GPU (H100/A100), обучение с нуля на триллионах токенов.
*   **Академия/Исследователи:** Один или несколько потребительских GPU (RTX 4090/3090).

**Вывод:** Эра обучения LLM с нуля для большинства закончилась. **Наш фокус — тонкая настройка (fine-tuning)** предобученных моделей.

> *«The days when regular Joe data scientists could train a language model from scratch are dead and gone.»* — Daniel Voigt Godoy

---

### **5. Память — главное ограничение**
Ограничивающий фактор — не FLOPs, а **объем видеопамяти (VRAM)**.

**Что хранится в VRAM во время обучения?**
1.  **Параметры модели** (в FP32 для оптимизатора, в FP16/BF16 для вычислений).
2.  **Градиенты** (того же размера, что и параметры).
3.  **Оптимизаторные состояния** (например, моменты для Adam, ~2x от параметров).
4.  **Активации** (зависит от длины последовательности и размера батча).

**Формула оценки памяти для полной настройки (Full Fine-Tuning):**
$\text{VRAM} \approx 4 * P * (1 + 2 + 2) + \text{Activations}$ (где $P$ — число параметров)

---

### **6. Численная иллюстрация проблемы**
| Модель | Параметры | FP32 (веса) | FP16 (веса) | Оценочная VRAM для FFT* |
|--------|-----------|-------------|-------------|--------------------------|
| Phi-3 Mini | 3.8B | ~14.4 GB | ~7.2 GB | **> 50 GB** |
| Llama 3 8B | 8B | ~32 GB | ~16 GB | **> 100 GB** |
| Llama 3 70B | 70B | ~280 GB | ~140 GB | **> 700 GB** |

*FFT — Full Fine-Tuning. Оценка включает параметры, градиенты и состояния оптимизатора.

**Вывод:** Полная настройка даже 8B модели недоступна на одном GPU.

---

### **Слайд 7: Следствие и необходимость новых методов**
**Проблема:** Мы не можем обновить **все** 7+ миллиардов параметров.

**Решение:** Разделяем задачу:
1.  **Сжать (Quantize):** Резко уменьшить объем памяти, занимаемый **базовой моделью**. Веса замораживаются.
2.  **Адаптировать (Adapt):** Добавить и обучить небольшой набор **новых параметров**, адаптирующих модель к задаче.

**➡️ Два ключевых метода: Квантование (Quantization) и LoRA.**

---

### **8. Обзор решений: общая схема работы**
```
[ Предобученная базовая модель (7B, FP32) ]
            ⬇️ Квантование (BitsAndBytes)
[ Квантованная модель (7B, NF4) ~ в 8 раз меньше ]
            ⬇️ Добавление адаптеров (PEFT)
[ Модель с LoRA адаптерами (7B NF4 + 0.03B FP32) ]
            ⬇️ Настройка на данных (SFTTrainer)
[ Адаптированная модель (сохраняются только веса LoRA) ]
```

**Практика:** Весь цикл можно выполнить на GPU с **8-24 GB VRAM**.  
[Пример кода: Глава 0](http://intelligentsystems:8888/lab/tree/Chapter0.ipynb)

---

### **9. Переход к квантованию**
**Ключевой вопрос:** Как работает сжатие модели с 32 бит на параметр до 4 бит? Что такое `NF4` и `FP4`?

**Практическая цель:** Научиться безопасно загружать квантованную модель с помощью `transformers` и `BitsAndBytesConfig`.

---

### **10. Архитектура памяти GPU/CPU**
*(Схема: CPU RAM, PCIe шина, GPU VRAM, тензорные ядра)*
*   **VRAM (HBM):** Быстрая, но ограниченная. Хранит модель и активации.
*   **Обмен через PCIe:** Узкое место. Стараемся минимизировать.
*   **Квантование** решает проблему **хранения весов**.
*   **Градиентный чекпоинтинг** (позже) решает проблему **хранения активаций**.

---

## **Часть 2: Квантование**

### **11. Идея «биннинга» (гистограммы)**
**Аналогия:** Уменьшение глубины цвета изображения (TrueColor 32-bit → Indexed 8-bit).
**Суть:** Сопоставить множество возможных значений (FP32) меньшему набору уровней (INT8/NF4).

```python
# Упрощенная суть квантования
original_weights = torch.randn(1000) # FP32, 4000 байт
quantized_indices = torch.randint(0, 16, (1000,)) # INT4, 500 байт
scale, zero_point = compute_scale_and_zp(original_weights)
dequantized_weights = scale * (quantized_indices - zero_point) # ~FP32
```

**Потеря информации:** Неизбежна, но управляема.

---

### **12. Формальная модель линейного квантования**
Для перехода от типа **высокой точности** (FP32) к **низкой** (INT8):

$$
\left [
Q(w) = \text{round}\left(\frac{w}{\text{scale}}\right) + \text{zero\_point}
\right ]
\left [
\text{scale} = \frac{\max(w) - \min(w)}{2^n - 1}
\right ]
\left [
\hat{w} = \text{scale} \times (Q(w) - \text{zero\_point})
\right ]
$$

Где $(n)$ — число бит (например, $8$), $(\hat{w})$ — де-квантованный вес.

**Проблема:** Распределение весов в LLM — нормальное, с **выбросами**. Линейное квантование плохо работает с выбросами.

---

### **13. Типы данных: от FP32 до NF4**
| Тип | Биты | Применение | Плюсы | Минусы |
|------|------|------------|-------|---------|
| **FP32** | 32 | Точные вы-я | Высокая точность | Огромный объем |
| **BF16** | 16 | Обучение | Широкий диапазон, стабильность | Меньшая точность |
| **FP16** | 16 | Инференс | Эффективность | Риск underflow/overflow |
| **INT8** | 8  | Квантование | x4, скорость | Только инференс? |
| **NF4/FP4** | 4  | QLoRA | **x8, можно обучать** | Спец. поддержка |

**NF4 (Normal Float 4):** Специальный 4-битный тип, оптимизированный под нормальное распределение весов LLM. Основа метода **QLoRA**.

---

### **14. Расчет выгоды от квантования**
Пусть $(P)$ — число параметров модели.

*   **FP32 модель:** $(S_{fp32} = 4P)$ байт
*   **INT8 модель:** $(S_{int8} = P)$ байт **(в 4 раза меньше)**
*   **NF4 модель:** $(S_{nf4} \approx 0.5P)$ байт **(в 8 раз меньше!)**

**Пример для Llama 3 8B:**
*   FP32: $8 \cdot 10^9 \times 4 \text{ B} = 32\text{ GB}$
*   NF4: $~4 \text{ GB}$ → Поместится на **RTX 3090 (24GB)** с запасом для обучения!

---

### **15. Погрешность квантования и её влияние**
Моделируем квантованный вес: $(\hat{w} = w + \varepsilon)$, где $(\varepsilon)$ — ошибка квантования.

Влияние на функцию потерь $(L)$ (линеаризация):
$$
\left [
\delta L \approx \nabla_w L \cdot \varepsilon = \sum_i \frac{\partial L}{\partial w_i} \varepsilon_i
\right]
$$

**Ключевое наблюдение:** Ошибка вносит больший вклад, где велики **градиенты** $(\partial L / \partial w)$.

---

### **16: Ключевой вывод 1 + статистика весов**
**Вывод:** Ошибка квантования наиболее критична для весов с **большими градиентами**.

**Хорошая новость:** В предобученных LLM:
1.  Большинство весов сосредоточено в узком диапазоне около нуля.
2.  Выбросы (очень большие по модулю веса) **редки**, но важны.
3.  Градиенты при дообучении также часто невелики.

➡️ Методы квантования должны **особо аккуратно** обрабатывать выбросы.

*(График из книги: распределение весов в слоях Phi-3)*

---

### **17. Group-Wise Квантование**
**Проблема:** Один scale для всей большой матрицы (например, 4096x4096) — грубое приближение из-за выбросов.

**Решение (Group-Wise):**
1.  Разбиваем матрицу на блоки (например, по 64 или 256 элементов).
2.  Для **каждого блока** вычисляем свои `scale` и `zero_point`.
3.  Квантуем блоки независимо.

**Результат:** Локально точное квантование, глобально — высокое сжатие.

---

### **18. NF4 (Normal Float 4)**
**Идея:** Использовать нелинейные (неравномерные) уровни квантования, которые лучше соответствуют **квантилям нормального распределения**.

**Как работает:**
1.  Теоретическое нормальное распределение $N(0,1)$ разбивается на $2^4 = 16$ интервалов с равной вероятностью.
2.  Значения-уровни выбираются как средние значения внутри этих интервалов.
3.  Веса модели масштабируются в диапазон $[-1, 1]$ и сопоставляются с ближайшим NF4-уровнем.

**Преимущество перед FP4:** Лучше представляет типичные значения весов, минимизируя ошибку.

---

### **Слайд 19: Сравнение методов квантования**
| Метод | Биты | Точность | Можно обучать? | Поддержка в HF |
|-------|------|----------|----------------|----------------|
| **FP32/BF16** | 16/32 | Исходная | Да | `torch_dtype=...` |
| **LLM.int8()** | 8 | Очень высокая | Нет (веса заморожены) | `load_in_8bit=True` |
| **QLoRA (NF4)** | 4 | Хорошая | **Да** (через адаптеры) | `load_in_4bit=True, bnb_4bit_quant_type="nf4"` |

**Рекомендация для тонкой настройки:** **QLoRA (4-bit NF4)** — оптимальный баланс сжатия и качества.

---

### **20. Инструмент: Библиотека `bitsandbytes`**
*   **Что это?** Пакет PyTorch, реализующий 8-битную и 4-битную квантование, а также 8-битные оптимизаторы.
*   **История:** Создана Тимом Деттмерсом (Tim Dettmers). Стала де-факто стандартом для квантования LLM в сообществе Hugging Face.
*   **Интеграция:** Прозрачно интегрирована в `transformers`. Вам не нужно вызывать её функции напрямую. Достаточно правильного `BitsAndBytesConfig`.

```
# Установка
!pip install bitsandbytes accelerate
```

**Роль `accelerate`:** Управляет распределением модели по устройствам (`device_map="auto"`).

---

### **21. Конфигурация: объект `BitsAndBytesConfig`**
```python
import torch
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                  # Включить 4-битное квантование
    bnb_4bit_quant_type="nf4",          # Тип квантования (nf4 или fp4)
    bnb_4bit_use_double_quant=True,     # Вложенное квантование (экономит память)
    bnb_4bit_compute_dtype=torch.bfloat16 # Тип для вычислений!
)
```
**Ключевой параметр:** `bnb_4bit_compute_dtype`.
*   `torch.float32`: Максимальная стабильность, но медленнее.
*   `torch.bfloat16` (рекомендуется): Оптимальный баланс скорости и стабильности, если GPU поддерживает.
*   `torch.float16`: Риск численной нестабильности.

---

### **22. Код загрузки квантованной модели**
```python
from transformers import AutoModelForCausalLM

model_id = "microsoft/Phi-3-mini-4k-instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,  # Передаём конфиг квантования
    device_map="auto",               # Автоматическое распределение по GPU/CPU
    torch_dtype=torch.bfloat16       # Тип данных НЕквантованных слоёв
)
```
**Важно:** Параметр `torch_dtype` определяет тип **остальных** слоёв (эмбеддинги, нормализации, головы). Для совместимости с `compute_dtype` часто используют `torch.bfloat16`.

[Практика: Глава 2, раздел "Loading a Quantized Model"](http://intelligentsystems:8888/lab/tree/Chapter2.ipynb#%D0%97%D0%B0%D0%B3%D1%80%D1%83%D0%B7%D0%BA%D0%B0-%D0%BA%D0%B2%D0%B0%D0%BD%D1%82%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D0%B9-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8)

---

### **23. Что происходит внутри? (Схема)**
**До квантования:** 
`nn.Linear(in_features=4096, out_features=4096)` → **132 МБ** (при FP16)
**После квантования:**
`bnb.nn.Linear4bit(in_features=4096, out_features=4096)` → **~16.5 МБ**
**Механизм:**
1.  Веса (`weight`) сжимаются до `int8`/`uint8` (хранятся как "индексы биннов").
2.  Сохраняются параметры масштабирования (`quant_state`, `absmax` и др.).
3.  При прямом проходе (`forward`):
    *   Веса **де-квантуются** на лету в `compute_dtype` (например, bfloat16).
    *   Вычисления происходят в `compute_dtype`.
    *   Результат может быть приведен обратно к `torch_dtype`.

---

### **24. Эффект на память**
```python
print(f"Память модели ДО квантования (оценка FP16): {7.2:.1f} GB")
print(f"Память модели ПОСЛЕ квантования: {model.get_memory_footprint() / 1e9:.2f} GB")

# Пример вывода для Phi-3 Mini 3.8B:
# Память модели ДО квантования (оценка FP16): 7.2 GB
# Память модели ПОСЛЕ квантования: 2.21 GB
```
**Экономия:** ~ **3.3 раза** относительно FP16 (и ~8 раз относительно FP32).

**Почему не ровно в 8 раз?** Неквантованные слои (эмбеддинги, нормализации, головы) остаются в `torch_dtype` (bf16/fp16).

---

### **25. Ограничение и переход**
```python
for name, param in model.named_parameters():
    if 'qkv_proj' in name:
        print(f"{name}: requires_grad = {param.requires_grad}")
        break
# Вывод: model.model.layers.0.self_attn.qkv_proj.weight: requires_grad = False
```
**Ключевой вывод:** Веса в `Linear4bit` слоях **заморожены** (`requires_grad=False`).

**Следствие:** Прямое распространение градиента и обновление этих весов невозможно.

**Вопрос:** Как же тогда обучать модель под новую задачу?

**Ответ:** Добавить **малые обучаемые адаптеры** к этим замороженным слоям. ➡️ **LoRA**

---

### **Слайд 26: Демо 1 — Успешная загрузка**
*(Скриншот из Google Colab или RunPod)*
```
===================================
Загружаем модель: microsoft/Phi-3-mini-4k-instruct
Конфигурация: 4-bit NF4, compute_dtype=bfloat16
===================================
Память модели: 2.21 GB
Модель успешно размещена на GPU:0 (NVIDIA RTX 4090)
===================================
```
**Практический вывод:** Модель на 3.8B параметров загружается и работает на GPU с 8+ GB VRAM.

---

### **27. Демо 2 — Визуализация распределения весов**
*(Два графика рядом)*
1.  **Гистограмма весов слоя `q_proj` в FP16 (до квантования):** Плавное нормальное распределение.
2.  **Гистограмма де-квантованных весов `Linear4bit` (после):** Дискретные "ступеньки" (16 уровней для NF4).

**Наблюдение:** Общая форма распределения сохранена, но точность каждого веса снижена до 4 бит.

---

### **28. Резюме по квантованию**
1.  **Экономия памяти 8x:** Ключевая технология для запуска LLM на потребительском GPU.
2.  **Контролируемая ошибка:** Методы вроде NF4 и Group-Wise минимизируют потерю качества.
3.  **Замороженное основание:** Квантованная модель — это **неизменяемый базис**, который нужно адаптировать с помощью **обучаемых надстроек (LoRA)**.

**Итог:** Мы решили проблему **хранения** большой модели. Теперь решим проблему её **адаптации**.

---

### **29. Резервный слайд: Решение проблем**
**Ошибка:** `AttributeError: 'Parameter' object has no attribute 'quant_state'`
*   **Причина:** Попытка обновить (замороженный) квантованный вес.
*   **Решение:** Использовать `prepare_model_for_kbit_training()` из `peft` и добавлять только LoRA-адаптеры.

**Ошибка:** `ValueError: ... please use `load_in_8bit` or `load_in_4bit` correctly.`
*   **Причина:** Неверный порядок загрузки или конфликт аргументов.
*   **Решение:** Убедиться, что `quantization_config` передан в `from_pretrained()` и `torch_dtype` совместим.

---

## **Часть 3: Low-Rank Adaptation (LoRA) — Теория**

### **Слайд 30: Идея LoRA: «Заплатка» к весам**
**Полная настройка (Full FT):** Обновляем всю матрицу весов $W \in \mathbb{R}^{d \times k}$.
$[ W_{new} = W_{old} + \Delta W_{full} ]$
**Проблема:** $\Delta W_{full}$ слишком велика (миллиарды параметров).

**Идея LoRA:** Ограничить пространство обновлений **низкоранговыми** матрицами.
$$
W_{new} = W_{old} + \Delta W, \quad \Delta W = B A, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}
$$
где **ранг** $r \ll min(d, k)$ (например, 8, 16, 32).

**Аналогия:** Не перестраиваем весь двигатель (W), а добавляем маленький чип-тюнер (BA).

---

### **31: Гипотеза низкого ранга**
**Интуиция:** При тонкой настройке на конкретную задачу, модель не нуждается в полной перенастройке всех нейронных связей.

**Гипотеза (Hu et al., 2021):** Обновления весов $\Delta W$ для адаптации имеют **низкий «интеллектуальный ранг»** — лежат в низкоранговом подпространстве.

*(Анимация/Схема: Большая матрица W, к которой добавляется тонкая "плёнка" BA)*

**Эмпирическое подтверждение:** LoRA часто достигает качества полной настройки при r=8-64, что составляет <0.5% от исходных параметров.

---

### **32. Формализация и количество параметров**
Пусть исходный слой: $y = Wx$, где $W \in \mathbb{R}^{d \times k}$.

С LoRA: $y = Wx + (\frac{\alpha}{r}) \cdot (B A) x$

*   $A \in \mathbb{R}^{r \times k}$ (проекция вниз)
*   $B \in \mathbb{R}^{d \times r}$ (проекция обратно)
*   $r$: ранг (гиперпараметр)
*   $\alpha$: коэффициент масштабирования (гиперпараметр, обычно $\alpha = 2r$).

**Число параметров:**
*   Полная настройка $W$: $P_{full} = d \times k$
*   LoRA (\(A\) и \(B\)): $P_{lora} = r \times (d + k)$

---

### **33. Пример: Численная выгода**
**Слой:** `d = k = 4096` (типичный размер в LLM).
*   **Полная настройка:** $P_{full} = 4096 \times 4096 = 16,777,216$ параметров (~16.8M).
*   **LoRA с r=16:** $P_{lora} = 16 \times (4096 + 4096) = 131,072$ параметров (~0.13M).

**Выгода:** $P_{lora} / P_{full} \approx 0.0078$ (**0.78%** от исходного числа!).

**Для всей модели (7B параметров):** Добавляем ~0.05B - 0.2B обучаемых параметров вместо обновления 7B.

---

### **34. Аппроксимационная способность (мотивация SVD)**
**Теоретическая основа:** Любую матрицу $\Delta W$ можно разложить как SVD:
$$
\Delta W = U \Sigma V^T = \sum_{i=1}^{R} \sigma_i u_i v_i^T
$$
где $\sigma_1 \ge \sigma_2 \ge ... \ge \sigma_R$ — сингулярные числа.
**Ключевое наблюдение:** Для многих матриц, соответствующих "естественным" обновлениям, энергия (сумма $\sigma_i^2$) сосредоточена в первых нескольких компонентах.

**График «Качество (Loss) vs. Ранг адаптера $(r)$»:**
*   Качество резко растет при увеличении $r$ от $1$ до $8$-$16$.
*   После $r=32$ прирост незначителен (насыщение).
*   **Вывод:** Низкого ранга достаточно для эффективной адаптации.

---

### **35. Влияние на оптимизацию**
**Вычисление градиентов:**
$$
\frac{\partial L}{\partial A} = \frac{\alpha}{r} \cdot B^T \frac{\partial L}{\partial y} x^T
$$
$$
\frac{\partial B}{\partial A} = \frac{\alpha}{r} \cdot \frac{\partial L}{\partial y} (A x)^T
$$
*   Градиенты имеют малую размерность ($r \times k$ и $d \times r$), что ускоряет вычисления.
*   **Память для оптимизатора (Adam):** Нужно хранить моменты только для $A$ и $B$, а не для всего $W$. Экономия >90%.

**Физический смысл:** $A$ учится "проектировать" вход $x$ в низкоранговое пространство. $B$ учится интерпретировать результат этой проекции.

---

### **36. Инициализация адаптеров**
Цель: В начале обучения модель с адаптерами должна вести себя так же, как базовая модель.
$$
y = Wx + (\frac{\alpha}{r}) B A x
$$
**Стандартная схема инициализации в PEFT:**
*   $A$ инициализируется случайным (но маленьким) шумом (e.g., $N(0, σ^2)$).
*   $B$ инициализируется **нулевой матрицей**.

**Следствие:** На первом шаге $BA = 0$, следовательно $y = Wx$. Обучение начинается "с нуля" без скачков.

---

### **37. Масштабирование: роль параметра `lora_alpha`**
Финальный выход слоя:
$$
output = Wx + \frac{\alpha}{r} \cdot (B A) x
$$
*   $\frac{\alpha}{r}$ — **коэффициент масштабирования**.
*   **Типичный выбор:** $\alpha = 2r$, тогда масштаб $= 2$. Эмпирически работает хорошо.
*   **Интерпретация:** Контролирует, насколько "голос" адаптера влияет на выход относительно оригинального веса \(W\).
*   **Эффект:** Можно использовать фиксированный масштаб, меняя только ранг $r$. Упрощает гиперпараметрический поиск.

---

### **38. Ограничения LoRA**
LoRA — не серебряная пуля.

**Когда LoRA может не сработать:**
1.  **Задачи, требующие высокоранговых изменений:** Например, обучение принципиально новым навыкам или знаниям.
2.  **Очень маленькие модели (<1B параметров):** Полная настройка дешевле.
3.  **Нестандартные архитектуры:** Ручной подбор `target_modules`.
4.  **Адаптация встраиваний (embeddings):** Прямая LoRA на эмбеддингах менее изучена, часто их просто дообучают полностно (добавляя в `modules_to_save`).

**Вывод:** LoRA идеально подходит для **стилевой адаптации, следования инструкциям, изучения узкой предметной области**.
[Практика: Глава 3](http://intelligentsystems:8888/lab/tree/Chapter3.ipynb)

---

### **39. Инструмент: Библиотека `peft` (PEFT)**
*   **Что это?** Пакет Hugging Face для **параметрически-эффективной тонкой настройки**.
*   **Философия:** Единый API для разных методов адаптации (LoRA, Prefix Tuning, IA³, AdaLoRA и др.).
*   **Интеграция:** Плотно интегрирована с `transformers` и `trl`.
*   **Ключевая абстракция:** `PeftModel` — обёртка вокруг исходной модели, управляющая адаптерами.

```python
# Установка
!pip install peft
```

---

### **40. Конфигурация: объект `LoraConfig`**
```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,                           # Ранг адаптеров
    lora_alpha=32,                  # Коэффициент масштабирования (обычно 2*r)
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"], # К каким слоям добавить LoRA
    lora_dropout=0.05,              # Dropout для адаптеров
    bias="none",                    # Не обучать bias (для совместимости)
    task_type="CAUSAL_LM"           # Тип задачи: языковое моделирование
)
```
**Выбор `target_modules`:**
*   Для современных decoder-only моделей (Llama, Mistral, Phi): `["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]`
*   Автоматический подбор для популярных архитектур через `TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING`.

---

### **41: Подготовка модели: `prepare_model_for_kbit_training()`**
```python
from peft import prepare_model_for_kbit_training
model = prepare_model_for_kbit_training(
    model,
    use_gradient_checkpointing=True  # Включить чекпоинтинг градиентов
)
```
**Что делает эта функция (важно!):**
1.  **Заморозит все параметры** базовой модели.
2.  **Приведёт нормализующие слои (LayerNorm/RMSNorm) к FP32** для численной стабильности.
3.  **Включит градиентный чекпоинтинг** (если передано `use_gradient_checkpointing=True`).
4.  **Настроит градиенты для эмбеддингов.**

[Практика: Глава 3, раздел "Parameter Types and Gradients"](http://intelligentsystems:8888/lab/tree/Chapter3.ipynb#%D0%A2%D0%B8%D0%BF%D1%8B-%D0%BF%D0%B0%D1%80%D0%B0%D0%BC%D0%B5%D1%82%D1%80%D0%BE%D0%B2-%D0%B8-%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D1%8B)

**Результат:** Модель готова к безопасному добавлению адаптеров.

---

### **42. Создание PEFT-модели**
```python
from peft import get_peft_model

peft_model = get_peft_model(model, lora_config)
```
**Что происходит:**
1.  Оригинальные `Linear` слои (или `Linear4bit`) оборачиваются в `LoraLayer`.
2.  Добавляются два маленьких обучаемых слоя (`lora_A`, `lora_B`).
3.  В прямом проходе: `output = base_layer(x) + lora_B(lora_A(x)) * (alpha / r)`.

**Важно:** Исходные веса модели остаются **замороженными**.

---

### **43. «Волшебный» момент: Проверка обучаемых параметров**
```python
peft_model.print_trainable_parameters()

# Пример вывода для модели 7B с r=16:
# trainable params: 8,847,360 || all params: 7,255,089,664 || trainable%: 0.1220
```
**Цифры говорят сами за себя:** Обучается **менее 0.2%** параметров модели!

**Память для оптимизатора (Adam):**  
Требуется хранить моменты только для ~9M параметров, а не для 7B.  
Экономия памяти оптимизатора: **~99.9%**.

---

### **44. Пайплайн QLoRA — Итоговая схема**
**QLoRA = 4-bit Quantization + LoRA**

**Схема прямого прохода для одного слоя:**
```
Вход (x) → [Квантованная база (W_4bit)] → Выход_базы
         → [Адаптер (A → B в FP16)] → Выход_адаптера * (alpha/r)
         ↓
Сумма: Выход_слоя = Де-квантованный(W_4bit) * x + (B * A) * x * (alpha/r)
```

**Практический результат:**  
На GPU с 24 ГБ VRAM можно настроить модель **70B** параметров, используя QLoRA!

---

### **45. Демо 3 — Код обучения с `SFTTrainer`**
```python
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
trainer = SFTTrainer(
    model=peft_model,
    args=SFTConfig(
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        max_seq_length=2048,
        learning_rate=2e-4,
        num_train_epochs=3,
        fp16=True,  # или bf16=True
        output_dir="./results",
        logging_steps=10,
    ),
    train_dataset=dataset,
    tokenizer=tokenizer,
    formatting_func=formatting_func,  # Функция форматирования данных
)
trainer.train()
```
**Почему `SFTTrainer`?**  
Упрощает настройку, автоматически обрабатывает:
*   Упаковку последовательностей (packing)
*   Сдвиг меток (label shifting)
*   Применение чат-шаблонов

[Практика: Глава 5, раздел "Fine-Tuning with SFTTrainer"](http://intelligentsystems:8888/lab/tree/Chapter5.ipynb)

---

### **46. Демо 4 — График сходимости (гипотетический)**
*(График: Loss по шагам обучения)*
*   **Синяя линия:** Full Fine-Tuning (7B параметров, быстро сходится, но требует 8x GPU)
*   **Оранжевая линия:** QLoRA (0.1% параметров, сходится почти так же хорошо!)
*   **Зеленая линия:** LoRA без квантования (больше параметров, чем QLoRA)

**Вывод:** QLoRA обеспечивает **сопоставимое качество** с полной настройкой при **на порядок меньших требованиях к памяти**.

---

### **47. Управление адаптерами**
**Сохранение и загрузка:**
```python
# Сохраняем только адаптеры (50 МБ вместо 14 ГБ)
peft_model.save_pretrained("./my_lora_adapter")

# Загружаем адаптеры на ту же базовую модель
from peft import PeftModel
loaded_model = PeftModel.from_pretrained(base_model, "./my_lora_adapter")
```

**Слияние адаптера с базовой моделью:**
```python
# Для развертывания: создаём единую модель
merged_model = peft_model.merge_and_unload()
merged_model.save_pretrained("./merged_model")
```

**Мульти-адаптеры:** Одна базовая модель + несколько адаптеров для разных задач.

---

### **48. Резюме по LoRA — 3 ключевых тезиса**
1.  **Обучение <1% параметров:** Революционное снижение вычислительных затрат.
2.  **Низкоранговая гипотеза верна:** Для адаптации LLM достаточно низкоранговых обновлений.
3.  **Синергия с квантованием:** QLoRA позволяет настраивать модели любого размера на потребительском GPU.

**Итог:** LoRA/QLoRA делают тонкую настройку LLM **демократичной** и доступной для исследовательских лабораторий и стартапов.

---

### **49. Резерв — Расширенные техники PEFT**
*   **DoRA (Weight-Decomposed Low-Rank Adaptation):** Разделяет обновление на величину и направление, часто лучше обычной LoRA.
*   **VeRA (Vector-based Random Matrix Adaptation):** Ещё более экономный метод — общие случайные матрицы для всех слоев.
*   **Адаптеры для эмбеддингов:** `target_modules=["embed_tokens"]` + специальная инициализация.
*   **Смешанные методы:** LoRA + полное обучение нормализаций (`modules_to_save=["norm", "lm_head"]`).

---

### **50. Важность данных (Часть 4)**
**«Модель ест токены. Плохие данные → плохие результаты.»**

Даже самая совершенная методика QLoRA не спасет от:
*   Некорректного форматирования (модель не понимает, что от неё хотят)
*   Низкого качества разметки (противоречивые примеры)
*   Несбалансированного датасета

**Правило 80/20 в тонкой настройке:**  
80% усилий — на сбор и подготовку данных, 20% — на настройку модели.

---

### **51. Шаблоны (Chat Templates) — Язык общения с моделью**
```jinja
<!-- Llama 3 -->
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{system_message}<|eot_id|>
<|start_header_id|>user<|end_header_id|>
{prompt}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
<!-- Mistral -->
[INST] {system_prompt} {user_message} [/INST]
<!-- Phi-3 -->
<|user|>
{prompt}<|end|>
<|assistant|>
```
[Практика: Глава 4, раздел "Applying Templates"](http://intelligentsystems:8888/lab/tree/Chapter4.ipynb#%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-%D1%88%D0%B0%D0%B1%D0%BB%D0%BE%D0%BD%D0%BE%D0%B2)

**Золотое правило:** Всегда используйте **родной чат-шаблон** модели, указанный в `tokenizer.chat_template`.
**Практика:** Применение шаблона через `tokenizer.apply_chat_template()`.

---

### **52. Итоги и выводы — Сводная таблица**
| Метрика | До применения методов | После QLoRA |
|---------|---------------------|-------------|
| **Память модели** | 28 ГБ (7B, FP32) | **3.5 ГБ** |
| **Обучаемые параметры** | 7B (100%) | **~10M (0.14%)** |
| **Память оптимизатора** | ~21 ГБ (Adam) | **~30 МБ** |
| **Требуемый GPU** | H100/A100 (80ГБ) | **RTX 3090/4090 (24ГБ)** |
| **Время эпохи** | Очень долго | **Часы, а не дни** |
| **Качество** | Эталонное | **Сопоставимое** |

**Ответ на ключевой вопрос (слайд 3):**  
Да, можно адаптировать LLM на одном GPU, используя **квантование (NF4)** для сжатия модели и **низкоранговые адаптеры (LoRA)** для её эффективной настройки.

---

### **53. Открытые вопросы — Темы для магистерских работ**
1.  **Оптимальный выбор ранга $r$ в зависимости от задачи:** Классификация текста vs. кодогенерация vs. диалог.
2.  **Адаптивные методы ранжирования:** Может ли модель сама определять, каким слоям нужен больший ранг?
3.  **QLoRA для мультимодальных моделей:** Эффективные методы настройки VLMs (например, LLaVA).
4.  **Безопасность и устойчивость LoRA-адаптеров:** Можно ли "взломать" модель через злонамеренный адаптер?
5.  **Дистилляция LoRA-адаптеров:** Компрессия нескольких адаптеров в один для multi-task модели.

---

### **54. Спасибо за внимание! Ваши вопросы?**

---

### **55. Контактная информация и ссылки**
*   **Книга:** Daniel Voigt Godoy. *"A Hands-On Guide to Fine-Tuning Large Language Models"* (2025)
*   **Репозиторий с кодом лекции:** [github.com/ваш-логик/FineTuning-LLM-Seminar](http://intelligentsystems:8888/lab)
*   **Практические демо:**
    *   [Глава 0: Быстрый старт](http://intelligentsystems:8888/lab/tree/Chapter0.ipynb)
    *   [Глава 2: Квантование](http://intelligentsystems:8888/lab/tree/Chapter2.ipynb)
    *   [Глава 3: LoRA](http://intelligentsystems:8888/lab/tree/Chapter3.ipynb)
    *   [Глава 5: Обучение с SFTTrainer](http://intelligentsystems:8888/lab/tree/Chapter5.ipynb)

---

### **56. Финальный резервный слайд**
*Готов ответить на вопросы по:*
- Устранению ошибок `CUDA out of memory`
- Выбору гиперпараметров для вашей конкретной задачи
- Развертыванию настроенной модели через Ollama/llama.cpp
- Этическим аспектам настройки LLM

**Удачи в экспериментах с тонкой настройкой!**
