{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глава 0: Краткое руководство\n",
    "\n",
    "### Предупреждение о спойлерах\n",
    "\n",
    "В этой короткой главе мы сразу перейдем к делу и выполним тонкую настройку небольшой языковой модели, а именно Microsoft Phi-3 Mini 4K Instruct, для перевода английского языка на манер речи Йоды. Эту начальную главу можно рассматривать как рецепт, которому можно просто следовать. Это глава в духе «сначала стреляй, потом задавай вопросы».\n",
    "\n",
    "Вы научитесь:\n",
    "*   загружать квантизированную модель с использованием `BitsAndBytes`;\n",
    "*   настраивать низкоранговые адаптеры (LoRA) с помощью `peft` от Hugging Face;\n",
    "*   загружать и форматировать набор данных;\n",
    "*   выполнять тонкую настройку модели с помощью `SFTTrainer` (supervised fine-tuning trainer) из библиотеки `trl` от Hugging Face;\n",
    "*   использовать дообученную модель для генерации нескольких предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка окружения\n",
    "\n",
    "Для обеспечения лучшей воспроизводимости во время обучения используйте зафиксированные версии, указанные ниже (те же, что использовались в книге):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оригинальные версии\n",
    "#!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4\n",
    "# Обновленные версии - Октябрь 2025\n",
    "# !pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вы используете Colab\n",
    "#!pip install datasets bitsandbytes trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вы используете Jupyter Template от runpod.io\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка квантизированной базовой модели\n",
    "\n",
    "Мы начнем с загрузки квантизированной модели, чтобы она занимала меньше места в оперативной памяти GPU. Квантизированная модель заменяет исходные веса приближенными значениями, представленными меньшим количеством бит. Самый простой и прямой способ квантизации модели — преобразовать ее веса из 32-битных чисел с плавающей запятой (FP32) в 4-битные числа с плавающей запятой (NF4). Это простое, но мощное изменение уже **сокращает объем памяти, занимаемый моделью**, примерно в восемь раз.\n",
    "\n",
    "Мы можем использовать экземпляр `BitsAndBytesConfig` в качестве аргумента `quantization_config` при загрузке модели с помощью метода `from_pretrained()`. Чтобы сохранить гибкость и позволить вам опробовать любую другую модель по вашему выбору, мы используем `AutoModelForCausalLM` от Hugging Face. Репозиторий, который вы выберете, определяет загружаемую модель.\n",
    "\n",
    "Без дальнейших церемоний, вот наша квантизированная модель, которая загружается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c246b10f9b674dfbba3dc15dacf5250b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote class=\"note\">\n",
    "  <p>\n",
    "    <em>\"Phi-3-Mini-4K-Instruct — это легкая, современная открытая модель с 3.8 млрд параметров, обученная на наборах данных Phi-3, которые включают как синтетические данные, так и отфильтрованные общедоступные данные веб-сайтов с акцентом на высокое качество и свойства, насыщенные логическими рассуждениями. Модель принадлежит к семейству Phi-3, мини-версия представлена в двух вариантах: 4K и 128K, что указывает на поддерживаемую длину контекста (в токенах).\"</em>\n",
    "    <br>\n",
    "    Источник: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">Hugging Face Hub</a>\n",
    "  </p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.341312\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже будучи квантизированной, модель по-прежнему занимает чуть более 2 гигабайт оперативной памяти. Процедура **квантизации** применяется к **линейным слоям внутри блоков Transformer-декодера** (в некоторых случаях также называемых \"слоями\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Квантизированную модель** можно использовать напрямую для вывода (инференса), но **невозможно продолжить ее обучение**. Эти назойливые слои `Linear4bit` занимают гораздо меньше места, что и является целью квантизации; однако мы не можем их обновлять.\n",
    "\n",
    "Нам нужно добавить кое-что еще в нашу смесь — немного адаптеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка низкоранговых адаптеров (LoRA)\n",
    "\n",
    "Низкоранговые адаптеры могут быть подключены к каждому из квантизированных слоев. **Адаптеры** по большей части представляют собой **обычные слои `Linear`**, которые можно легко обновлять обычным образом. Хитрый трюк здесь заключается в том, что эти адаптеры **значительно меньше** по размеру, чем квантизированные слои.\n",
    "\n",
    "Поскольку **квантизированные слои заморожены** (их нельзя обновлять), настройка **адаптеров LoRA** на квантизированной модели резко **сокращает общее количество обучаемых параметров** до всего 1% (или менее) от исходного размера.\n",
    "\n",
    "Мы можем настроить адаптеры LoRA в три простых шага:\n",
    "\n",
    "*   Вызвать `prepare_model_for_kbit_training()` для *повышения численной стабильности* во время обучения.\n",
    "*   Создать экземпляр `LoraConfig`.\n",
    "*   Применить конфигурацию к квантизированной базовой модели с помощью метода `get_peft_model()`.\n",
    "\n",
    "Давайте попробуем сделать это с нашей моделью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,                   # ранг адаптера; чем меньше, тем меньше параметров нужно обучать\n",
    "    lora_alpha=16,         # множитель, обычно 2*r\n",
    "    bias=\"none\",           # ВНИМАНИЕ: обучение смещений *изменяет* поведение базовой модели\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Более новые модели, такие как Phi-3 на момент написания, могут требовать\n",
    "    # ручной установки целевых модулей\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод для остальных трех слоев LoRA (`qkv_proj`, `gate_up_proj` и `down_proj`) был скрыт для сокращения вывода.\n",
    "\n",
    "<blockquote class=\"warning\">\n",
    "  <p>\n",
    "    Возникла ли у вас следующая ошибка?\n",
    "    <br>\n",
    "    <br>\n",
    "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
    "    <br>\n",
    "    <br>\n",
    "    Скорее всего, вам не нужно указывать <tt>target_modules</tt>, если вы используете одну из хорошо известных моделей. Библиотека <tt>peft</tt> позаботится об этом, *автоматически выбирая подходящие цели*. Однако может возникнуть разрыв между временем выпуска популярной модели и обновлением библиотеки. Поэтому, если вы получили указанную выше ошибку, найдите квантизированные слои в вашей модели и перечислите их имена в аргументе <tt>target_modules</tt>.\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "Квантизированные слои (`Linear4bit`) превратились в модули `lora.Linear4bit`, где сам квантизированный слой стал `base_layer` с добавленными к нему обычными слоями `Linear` (`lora_A` и `lora_B`).\n",
    "\n",
    "Эти дополнительные слои должны были бы лишь незначительно увеличить модель. Однако **функция подготовки модели** (`prepare_model_for_kbit_training()`) преобразовала **все неквантизированные слои в полную точность (FP32)**, что привело к увеличению размера модели на 30%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.074752\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку большинство параметров заморожено, лишь крошечная доля от общего количества параметров в настоящее время является обучаемой, благодаря LoRA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаемые параметры:             12.58M\n",
      "Всего параметров:                3833.66M\n",
      "Доля обучаемых параметров:       0.33%\n"
     ]
    }
   ],
   "source": [
    "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
    "print(f'Обучаемые параметры:             {trainable_parms/1e6:.2f}M')\n",
    "print(f'Всего параметров:                {tot_parms/1e6:.2f}M')\n",
    "print(f'Доля обучаемых параметров:       {100*trainable_parms/tot_parms:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель готова к тонкой настройке, но нам все еще не хватает одного ключевого компонента: нашего набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Форматирование вашего набора данных\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"Как Йода, говорить, ты должен. Хррммм.\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    Мастер Йода\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "Набор данных [`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) состоит из 720 предложений, переведенных с английского на манер речи Йоды. Набор данных размещен на Hugging Face Hub, и мы можем легко загрузить его с помощью метода `load_dataset()` из библиотеки Hugging Face `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных имеет три столбца:\n",
    "\n",
    "*   исходное предложение на английском (`sentence`);\n",
    "*   базовый перевод на манер речи Йоды (`translation`);\n",
    "*   расширенный перевод, включающий типичные междометия `Yesss` и `Hrrmm` (`translation_extra`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFTTrainer`, который мы будем использовать для тонкой настройки модели, может автоматически обрабатывать наборы данных в **диалоговом** формате.\n",
    "\n",
    "```\n",
    "{\"messages\":[\n",
    "  {\"role\": \"system\", \"content\": \"<общие директивы>\"},\n",
    "  {\"role\": \"user\", \"content\": \"<текст промпта>\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"<идеально сгенерированный текст>\"}\n",
    "]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**ВАЖНОЕ ОБНОВЛЕНИЕ**: К сожалению, в более новых версиях библиотеки `trl` формат \"instruction\" больше не поддерживается должным образом, что приводит к тому, что шаблон чата не применяется к набору данных. Чтобы избежать этой проблемы, мы можем преобразовать набор данных в \"диалоговый\" формат.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Адаптировано из trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Преобразует набор данных из формата промпт/завершение (больше не поддерживается)\n",
    "# в диалоговый формат\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
       " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\n",
    "messages = dataset[0]['messages']\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизатор\n",
    "\n",
    "Прежде чем перейти к непосредственному обучению, нам все еще необходимо **загрузить токенизатор, соответствующий нашей модели**. Токенизатор является важной частью этого процесса, определяя, как преобразовывать текст в токены тем же способом, который использовался при обучении модели.\n",
    "\n",
    "Для инструктивных/чат-моделей токенизатор также содержит соответствующий **шаблон чата**, который определяет:\n",
    "\n",
    "*   Какие **специальные токены** следует использовать и где их размещать.\n",
    "*   Где должны размещаться системные директивы, промпт пользователя и ответ модели.\n",
    "*   Что такое **промпт генерации**, то есть специальный токен, который запускает ответ модели (подробнее об этом в разделе \"Запрос к модели\").\n",
    "\n",
    "***\n",
    "\n",
    "**ВАЖНОЕ ОБНОВЛЕНИЕ**: Из-за изменений в стандартном сборщике (collator), используемом классом `SFTTrainer` при построении набора данных, токен EOS (который в Phi-3 совпадает с токеном PAD) также маскировался в метках, что приводило к невозможности корректной остановки генерации токенов моделью.\n",
    "\n",
    "Чтобы устранить это изменение, мы можем назначить токен UNK в качестве токена PAD, чтобы токен EOS стал уникальным и, следовательно, не маскировался как часть меток.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3211de4cdb73493f8a16ad18e2a2fe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054fb60e85954ac4ab7f7d17d9eecc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bcfd5fe33c4486be133325abfce907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2039a242dc34e2ab2c0218dbb73f507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Загружаем все файлы в локальную директорию\n",
    "snapshot_download(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    local_dir=\"./phi3-mini-tokenizer\",\n",
    "    allow_patterns=[\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"added_tokens.json\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Затем загружаем из локальной директории\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi3-mini-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "tokenizer.chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не обращайте внимания на кажущийся излишне сложным шаблон (я добавил переносы строк и отступы, чтобы его было легче читать). Он просто организует сообщения в связный блок с соответствующими тегами, как показано ниже (`tokenize=False` гарантирует, что мы получим читаемый текст вместо числовой последовательности идентификаторов токенов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что каждое взаимодействие обрамлено либо токенами `<|user|>`, либо `<|assistant|>` в начале и `<|end|>` в конце. Более того, токен `<|endoftext|>` указывает на конец всего блока.\n",
    "\n",
    "У разных моделей будут разные шаблоны и токены для обозначения начала и конца предложений и блоков.\n",
    "\n",
    "Теперь мы готовы приступить к собственно тонкой настройке!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тонкая настройка с помощью SFTTrainer\n",
    "\n",
    "**Тонкая настройка модели**, большой или не очень, следует точно **той же процедуре обучения, что и обучение модели с нуля**. Мы могли бы написать собственный цикл обучения на чистом PyTorch или использовать `Trainer` от Hugging Face для тонкой настройки нашей модели.\n",
    "\n",
    "Однако гораздо проще использовать `SFTTrainer` (который, кстати, использует `Trainer` внутри), поскольку он позаботится о большинстве мелких деталей за нас, при условии, что мы предоставим ему следующие четыре аргумента:\n",
    "\n",
    "*   модель;\n",
    "*   токенизатор;\n",
    "*   набор данных;\n",
    "*   объект конфигурации.\n",
    "\n",
    "Первые три элемента у нас уже есть; давайте поработаем над последним.\n",
    "\n",
    "### SFTConfig\n",
    "\n",
    "Существует множество параметров, которые мы можем установить в объекте конфигурации. Мы разделили их на четыре группы:\n",
    "\n",
    "*   Параметры оптимизации **использования памяти**, связанные с **накоплением градиентов и контрольными точками**;\n",
    "*   Аргументы, связанные с **набором данных**, такие как `max_seq_length`, требуемая вашими данными, и упаковываете ли вы последовательности;\n",
    "*   Типичные **параметры обучения**, такие как `learning_rate` и `num_train_epochs`;\n",
    "*   Параметры **окружения и логирования**, такие как `output_dir` (это будет имя модели, если вы решите загрузить ее на Hugging Face Hub после обучения), `logging_dir` и `logging_steps`.\n",
    "\n",
    "Хотя *скорость обучения* является очень важным параметром (в качестве отправной точки можно попробовать скорость обучения, использовавшуюся при первоначальном обучении базовой модели), именно **максимальная длина последовательности** с большей вероятностью может вызвать **проблемы с нехваткой памяти**.\n",
    "\n",
    "Всегда выбирайте минимально возможную `max_seq_length`, которая имеет смысл для вашего случая использования. В нашем случае предложения — как на английском, так и в манере речи Йоды — довольно короткие, и последовательности из 64 токенов более чем достаточно, чтобы охватить промпт, завершение и добавленные специальные токены.\n",
    "\n",
    "<blockquote class=\"tip\">\n",
    "  <p>\n",
    "    Flash Attention (который, к сожалению, не поддерживается в Colab) обеспечивает большую гибкость при работе с длинными последовательностями, позволяя избежать потенциальной проблемы ошибок нехватки памяти (OOM).\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "***\n",
    "\n",
    "**ВАЖНОЕ ОБНОВЛЕНИЕ**: Выпуск `trl` версии 0.20 принес несколько изменений в `SFTConfig`:\n",
    "*   упаковка выполняется иначе, чем раньше, если не установлено `packing_strategy='wrapped'`;\n",
    "*   аргумент `max_seq_length` был переименован в `max_length`;\n",
    "*   `bf16` по умолчанию имеет значение `True`, но на момент этого обновления (октябрь 2025) не проверялось, доступен ли тип BF16 на самом деле или нет, поэтому он теперь включен в конфигурацию.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## ГРУППА 1: Использование памяти\n",
    "    # Эти аргументы позволят максимально эффективно использовать оперативную память вашего GPU\n",
    "    # Контрольные точки градиента\n",
    "    gradient_checkpointing=True,\n",
    "    # это экономит ОЧЕНЬ много памяти\n",
    "    # Установите это, чтобы избежать исключений в новых версиях PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # Накопление градиента / Размер пакета\n",
    "    # Фактический пакет (для обновления) такой же (1x), как размер микро-пакета\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Начальный размер (микро-)пакета\n",
    "    per_device_train_batch_size=16,\n",
    "    # Если размер пакета вызывает OOM, он уменьшается вдвое, пока не заработает\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## ГРУППА 2: Связанные с набором данных\n",
    "    max_length=64, # переименован в v0.20\n",
    "    # Набор данных\n",
    "    # Упаковка набора данных означает, что заполнение не требуется\n",
    "    packing=True,\n",
    "    packing_strategy='wrapped', # добавлено для приближения к исходному поведению упаковки\n",
    "\n",
    "    ## ГРУППА 3: Типичные параметры обучения\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # Оптимизатор\n",
    "    # 8-битный оптимизатор Adam - не сильно помогает, если вы используете LoRA!\n",
    "    optim='paged_adamw_8bit',\n",
    "\n",
    "    ## ГРУППА 4: Параметры логирования\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none',\n",
    "\n",
    "    # гарантирует, что bf16 (новый стандарт) используется только когда он действительно доступен\n",
    "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SFTTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SFTTrainer`\n",
    "\n",
    "<blockquote style=\"quotes: none !important;\">\n",
    "  <p>\n",
    "    <em>\"Настало время тренировки!\"</em>\n",
    "    <br>\n",
    "    <br>\n",
    "    Халк\n",
    "  </p>\n",
    "</blockquote>\n",
    "\n",
    "***\n",
    "\n",
    "**ВАЖНОЕ ОБНОВЛЕНИЕ**: До версии 0.23 `trl` существовала известная проблема, когда обучение завершалось неудачей, если конфигурация LoRA уже была применена к модели, поскольку тренер замораживал всю модель, включая адаптеры.\n",
    "\n",
    "Если модель уже содержала адаптеры, как в нашем случае, обучение работало только при использовании базовой исходной модели (`model.base_model.model`) вместе с аргументом `peft_config`.\n",
    "\n",
    "Эта проблема была исправлена в версии 0.23.1 `trl`, выпущенной в октябре 2025 года.\n",
    "\n",
    "***\n",
    "\n",
    "Теперь мы можем, наконец, создать экземпляр тренера для обучения с учителем (supervised fine-tuning trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'),\n",
       " tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "         32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "          5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "          1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "           935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "         32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "         29889, 32007, 32001,   512], device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFTTrainer` уже предварительно обработал наш набор данных, поэтому мы можем заглянуть внутрь и посмотреть, как был собран каждый мини-пакет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 01:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.8383120309222828, metrics={'train_runtime': 100.7582, 'train_samples_per_second': 33.843, 'train_steps_per_second': 2.183, 'total_flos': 4890970340720640.0, 'train_loss': 0.8383120309222828, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запрос к модели\n",
    "\n",
    "Теперь наша модель должна быть способна генерировать предложения в стиле Йоды в ответ на любое короткое предложение, которое мы ей дадим.\n",
    "\n",
    "Итак, модель требует, чтобы ее входные данные были правильно отформатированы. Нам нужно построить список \"сообщений\" — в данном случае от `user` — и предложить модели ответить, указав, что сейчас ее очередь писать.\n",
    "\n",
    "Для этого служит аргумент `add_generation_prompt`: он добавляет `<|assistant|>` в конец диалога, чтобы модель могла предсказать следующее слово — и продолжать делать это, пока не предскажет токен `<|endoftext|>`.\n",
    "\n",
    "Вспомогательная функция ниже формирует сообщение (в диалоговом формате) и **применяет шаблон чата** к нему, **добавляя промпт генерации** в его конец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем сгенерировать промпт для примера предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in you!<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Промпт выглядит правильным; давайте используем его для генерации завершения. Вспомогательная функция ниже выполняет следующие действия:\n",
    "\n",
    "*   **Токенизирует промпт** в тензор идентификаторов токенов (`add_special_tokens` установлен в `False`, потому что токены уже были добавлены шаблоном чата).\n",
    "*   Устанавливает модель в **режим оценки**.\n",
    "*   Вызывает метод `generate()` модели для **создания вывода** (сгенерированных идентификаторов токенов).\n",
    "    *   Если модель обучалась с использованием смешанной точности, мы оборачиваем генерацию в менеджер контекста `autocast()`, который автоматически обрабатывает преобразование между типами данных.\n",
    "*   **Декодирует сгенерированные идентификаторы токенов** обратно в читаемый текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # если обучение проводилось со смешанной точностью, используем контекст autocast\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:  \n",
    "        generation_output = model.generate(**tokenized_input,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем, наконец, опробовать нашу модель и посмотреть, действительно ли она способна генерировать речь в стиле Йоды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is! Yes, hrrrm.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потрясающе! Это работает! Модель говорит как Йода. Хрррммм.\n",
    "\n",
    "Поздравляю, вы выполнили тонкую настройку своей первой LLM!\n",
    "\n",
    "Теперь у вас есть небольшой адаптер, который можно загрузить в экземпляр модели Phi-3 Mini 4K Instruct, чтобы превратить ее в переводчика на язык Йоды! Как это круто?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение адаптера\n",
    "\n",
    "После завершения обучения вы можете сохранить адаптер (и токенизатор) на диск, вызвав метод `save_model()` тренера. Он сохранит все в указанную папку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('local-phi3-mini-yoda-adapter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраненные файлы включают:\n",
    "\n",
    "*   конфигурацию адаптера (`adapter_config.json`) и веса (`adapter_model.safetensors`) — сам адаптер занимает всего 50 МБ;\n",
    "*   аргументы обучения (`training_args.bin`);\n",
    "*   токенизатор (`tokenizer.json` и `tokenizer.model`), его конфигурацию (`tokenizer_config.json`) и специальные токены (`added_tokens.json` и `speciak_tokens_map.json`);\n",
    "*   файл README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer_config.json',\n",
       " 'adapter_model.safetensors',\n",
       " 'special_tokens_map.json',\n",
       " 'adapter_config.json',\n",
       " 'training_args.bin',\n",
       " 'chat_template.jinja',\n",
       " 'README.md',\n",
       " 'tokenizer.json']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('local-phi3-mini-yoda-adapter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы хотите поделиться своим адаптером со всеми, вы также можете загрузить его на Hugging Face Hub. Сначала войдите в систему, используя токен с разрешением на запись:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведенный выше код попросит вас ввести токен доступа:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
    "<center>Рисунок 0.1 - Вход в Hugging Face Hub</center>\n",
    "\n",
    "Успешный вход должен выглядеть так (обратите внимание на разрешения):\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
    "<center>Рисунок 0.2 - Успешный вход</center>\n",
    "\n",
    "Затем вы можете использовать метод `push_to_hub()` тренера, чтобы загрузить все в свою учетную запись в Hub. Модель будет названа в соответствии с аргументом `output_dir` аргументов обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! Наша модель теперь доступна всем, и любой может использовать ее для перевода английского языка на манер речи Йоды.\n",
    "\n",
    "На этом все!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchbook] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
