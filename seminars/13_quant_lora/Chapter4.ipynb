{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Глава 4: Форматирование набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Содержание главы\n",
    "\n",
    " В этой главе мы:\n",
    "\n",
    " *   Поймем важность определения корректного шаблона чата.\n",
    " *   Обсудим несколько альтернативных подходов к форматированию, включая пользовательские функции форматирования и шаблоны.\n",
    " *   Настроим токенизатор и слой эмбеддингов модели.\n",
    " *   Изучим упакованные наборы данных и различные сборщики данных (collators) для их загрузки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Настройка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (0.48.2)\n",
      "Requirement already satisfied: trl in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: filelock in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.1)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from trl) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: psutil in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (7.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.21.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Если вы используете Colab\n",
    "!pip install datasets bitsandbytes trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вы используете Jupyter Template от runpod.io\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DataCollatorForLanguageModeling, DataCollatorWithPadding, DataCollatorWithFlattening, BitsAndBytesConfig\n",
    "from trl import setup_chat_format #, DataCollatorForCompletionOnlyLM - удален в версии 0.20\n",
    "from trl.data_utils import pack_dataset\n",
    "from trl.extras.dataset_formatting import FORMAT_MAPPING, instructions_formatting_function, conversations_formatting_function\n",
    "# from trl.trainer import ConstantLengthDataset - удален в версии 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-11 16:22:00--  https://raw.githubusercontent.com/dvgodoy/FineTuningLLMs/refs/heads/main/compatibility_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15950 (16K) [text/plain]\n",
      "Saving to: ‘compatibility_functions.py.5’\n",
      "\n",
      "compatibility_funct 100%[===================>]  15.58K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2025-12-11 16:22:01 (1.93 MB/s) - ‘compatibility_functions.py.5’ saved [15950/15950]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Если вы используете Colab, необходимо загрузить замену удаленных функций\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/FineTuningLLMs/refs/heads/main/compatibility_functions.py\n",
    "\n",
    "from compatibility_functions import DataCollatorForCompletionOnlyLM, ConstantLengthDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Цель форматирования\n",
    "\n",
    " Мы форматируем набор данных, чтобы предоставить структуру и контекстные указания LLM. Мы можем легко направлять ее поведение (например, при инструктивной настройке), аккуратно оборачивая каждый компонент — промпт пользователя и ответ модели — соответствующими тегами и специальными токенами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Суть форматирования\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/base_prompt.png?raw=True)\n",
    "\n",
    " <center>Рисунок 4.1 - Предсказание следующего токена базовой моделью</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/fine_tuned_prompt.png?raw=True)\n",
    "\n",
    " <center>Рисунок 4.2 - Настроенная модель, активируемая шаблоном ответа</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_prompt_new.png?raw=True)\n",
    "\n",
    " <center>Рисунок 4.3 - Чат-модель, использующая шаблон чата</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_example_new.png?raw=True)\n",
    "\n",
    " <center>Рисунок 4.4 - Общая структура шаблона чата</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Итог предыдущих глав"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loschilov_aa/.conda/envs/pytorchbook/lib/python3.10/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce76521e6c3548d4aaf8ffad579d58d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download entire repository\n",
    "snapshot_download(\n",
    "    repo_id=\"facebook/opt-350m\",\n",
    "    local_dir=\"./opt-350m-model\",\n",
    "    ignore_patterns=[\"*.h5\", \"*.ot\", \"*.msgpack\"],  # Skip unnecessary formats\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "# Load from local directory\n",
    "model_dir = \"./opt-350m-model\"\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                                device_map='cuda:0',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)\n",
    "\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Применение шаблонов\n",
    "\n",
    " ****\n",
    " **Итог раздела \"Применение шаблонов\"**\n",
    "\n",
    "\n",
    "\n",
    " У вас есть три варианта форматирования набора данных:\n",
    "\n",
    " 1. Ваш набор данных представлен в **одном из двух форматов, поддерживаемых классом `STTrainer`** (диалоговый или инструктивный):\n",
    "    *   Ваш **токенизатор должен иметь настроенный шаблон чата**.\n",
    "    *   Не нужно определять функцию форматирования или форматировать набор данных перед обучением.\n",
    "    *   **ВАЖНО**: **инструктивный формат больше не поддерживается должным образом в последних версиях пакета `trl`**.\n",
    " 2. Вы хотите использовать **пользовательскую функцию форматирования** (см. \"BYOFF, Bring Your Own Formatting Function\"):\n",
    "    *   Пользовательская функция должна быть предоставлена как **аргумент `formatting_func` класса `SFTTrainer`** (см. Главу 5).\n",
    "    *   Ваша функция форматирования **должна обрабатывать батчи данных**.\n",
    "      *   Протестируйте ее, вызвав метод `map()` набора данных с `batched=True`.\n",
    "     *   Не нужно применять функцию к набору данных перед обучением.\n",
    "     *   Если ваш токенизатор уже **имеет шаблон чата**:\n",
    "       *   Вы можете вызвать его метод `apply_chat_template()` в своей функции.\n",
    "       *   Придерживайтесь общего формата шаблона (шаблоны инструкции и ответа).\n",
    "       *   Если шаблон не включает его, **вы можете добавить токен `EOS` в конец отформатированного вывода**.\n",
    "    *   Если ваш токенизатор **не имеет шаблона чата**:\n",
    "      *   Вы свободны в определении общего формата, включая шаблоны инструкции и ответа (см. \"Продвинутый уровень — BYOT, Bring Your Own Template\").\n",
    "\n",
    " 3. Ваш набор данных **уже отформатирован** (см. \"BYOFD, Bring Your Own Formatted Data\"):\n",
    "    *   Столбец, содержащий отформатированные данные, должен быть предоставлен как **аргумент `dataset_text_field` класса `SFTTrainer`** (см. Главу 5).\n",
    "    *   Даже если вы можете использовать свою функцию форматирования для предобработки набора данных, она не будет использоваться классом тренера.\n",
    "    *   Убедитесь, что ваши **данные совместимы с шаблоном токенизатора**.\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6e518d6ebf464ca8b9f2b8b91a0c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Загружаем все файлы в локальную директорию\n",
    "snapshot_download(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    local_dir=\"./phi3-mini-tokenizer\",\n",
    "    allow_patterns=[\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"added_tokens.json\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Затем загружаем из локальной директории\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"./phi3-mini-tokenizer\")\n",
    "print(tokenizer_phi.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
    "    {'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
    "    {'role': 'assistant', 'content': 'Buenos Aires.'}\n",
    "]\n",
    "\n",
    "formatted = tokenizer_phi.apply_chat_template(conversation=messages,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=False)\n",
    "print(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_input = tokenizer_phi.apply_chat_template(conversation=messages[:-1],\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "print(inference_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Поддерживаемые форматы\n",
    "\n",
    " ##### Диалоговый формат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': List({'content': Value('string'), 'role': Value('string')})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_ds = Dataset.from_list([{'messages': messages}])\n",
    "conversation_ds.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FORMAT_MAPPING['chatml'] == conversation_ds.features['messages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "What is the capital of Argentina?<|end|>\n",
      "<|assistant|>\n",
      "Buenos Aires.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
    "\n",
    "print(formatting_func(conversation_ds[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```python\n",
    " # функция форматирования для диалогового формата\n",
    " def format_dataset(examples):\n",
    "     if isinstance(examples[messages_field][0], list):\n",
    "         output_texts = []\n",
    "         for i in range(len(examples[messages_field])):\n",
    "             output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
    "\n",
    "         return output_texts\n",
    "     else:\n",
    "         return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Инструктивный формат\n",
    "\n",
    " ****\n",
    "\n",
    " **ВАЖНОЕ ОБНОВЛЕНИЕ**: к сожалению, в более новых версиях библиотеки `trl` инструктивный формат больше не поддерживается должным образом, что приводит к тому, что шаблон чата не применяется к набору данных. Чтобы избежать этой проблемы, рекомендуется использовать диалоговый формат.\n",
    "\n",
    " ****\n",
    "\n",
    "\n",
    "\n",
    " ```python\n",
    " instructions = [{'prompt': 'What is the capital of Argentina?',\n",
    "                  'completion': 'Buenos Aires.'}]\n",
    "\n",
    " instruction_ds = Dataset.from_list(instructions)\n",
    " instruction_ds.features\n",
    "\n",
    " {'prompt': Value('string'), 'completion': Value('string')}\n",
    "\n",
    " FORMAT_MAPPING['instruction'] == instruction_ds.features\n",
    "\n",
    " True\n",
    "\n",
    " formatting_func = instructions_formatting_function(tokenizer_phi)\n",
    " formatting_func\n",
    "\n",
    "\n",
    " trl.extras.dataset_formatting.instructions_formatting_function.<locals>.format_dataset\n",
    " def format_dataset(examples)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Адаптировано из trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Преобразует набор данных из формата промпт/завершение (больше не поддерживается)\n",
    "# в диалоговый формат\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts_completions = {\n",
    "    'prompt': ['What is the capital of Argentina?',\n",
    "               'What is the capital of the United States?'],\n",
    "    'completion': ['Buenos Aires.',\n",
    "                    'Washington D.C.']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
       "  {'role': 'assistant', 'content': 'Buenos Aires.'}],\n",
       " [{'role': 'user', 'content': 'What is the capital of the United States?'},\n",
       "  {'role': 'assistant', 'content': 'Washington D.C.'}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = format_dataset(batch_prompts_completions)['messages']\n",
    "batch_messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### BYOFF (Принеси свою собственную функцию форматирования)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func1(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d8061136aa43b4ba80cc67cca4717c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_msg = Dataset.from_dict({'messages': batch_messages})\n",
    "ds_msg.map(lambda v: tokenizer_phi(byo_formatting_func1(v)), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func2(examples):\n",
    "    response_template = '### Answer:'\n",
    "    text = f\"### Question: {examples['prompt']}\\n{response_template} {examples['completion']}\"\n",
    "    text += tokenizer_phi.eos_token\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the capital of Argentina?\n",
      "### Answer: Buenos Aires.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "ds_prompt = Dataset.from_dict(batch_prompts_completions)\n",
    "print(byo_formatting_func2(ds_prompt[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это вызовет исключение\n",
    "# ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func2(v)), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byo_formatting_func3(examples):\n",
    "    output_texts = []\n",
    "    response_template = '### Answer:'\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"### Question: {examples['prompt'][i]}\\n {response_template} {examples['completion'][i]}\"\n",
    "        text += tokenizer_phi.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeb3b224a4643598c1fc2e7ac669589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func3(v)), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### BYOFD (Принеси свои собственные отформатированные данные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byofd_formatting_func(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {'text': output_texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec717615a0e54df0ad6183ae01d95302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Column(['<|user|>\\nWhat is the capital of Argentina?<|end|>\\n<|assistant|>\\nBuenos Aires.<|end|>\\n<|endoftext|>', '<|user|>\\nWhat is the capital of the United States?<|end|>\\n<|assistant|>\\nWashington D.C.<|end|>\\n<|endoftext|>'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ds = ds_msg.map(byofd_formatting_func, batched=True)\n",
    "formatted_ds['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Сводка выбора формата\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/formatting_flow.png?raw=True)\n",
    "\n",
    "\n",
    "\n",
    " <center>Рисунок 4.5 - Выбор правильной конфигурации для ваших потребностей в форматировании</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Токенизатор\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"Токенизатор\"**\n",
    "\n",
    " *   **Словарь токенизатора** обычно **короче, чем слой эмбеддингов модели**.\n",
    "     *   Разница в размере состоит, буквально, из «пустых слотов», которые можно использовать для **создания новых токенов без изменения размера** слоя эмбеддингов.\n",
    "     *   **Размер слоя эмбеддингов** часто является **кратным степени двойки** (32, 64 и т.д.) для оптимизации **выделения памяти**.\n",
    " *   Токен `EOS` должен **использоваться исключительно для обозначения конца текста** и ничего более.\n",
    "     *   Использование токена `EOS` для дополнения (padding) может привести к _бесконечной генерации токенов_.\n",
    " *   Токен `PAD` часто не определен, но он может понадобиться:\n",
    "     *   **НЕ** назначайте токен `EOS` в качестве токена `PAD`.\n",
    "     *   Если токен `UNK` определен, можно назначить его в качестве токена `PAD`.\n",
    "     *   Если токен `UNK` не определен, создайте новый специальный токен в качестве токена `PAD`.\n",
    "     *   **ВНИМАНИЕ**: Если токен `PAD` остался **неопределенным**, многие библиотеки **по умолчанию назначат ему токен `EOS`**!\n",
    " *   Для **генеративных** моделей **дополнение** должно выполняться с **левой** стороны.\n",
    "     *   Дополнение _справа_ научит модель генерировать _бесконечные последовательности токенов дополнения_.\n",
    "     *   Во многих руководствах используется `tokenizer.padding_side='right'` из-за сообщений о проблемах с переполнением в классе `SFTTrainer`.\n",
    "       *   Это допустимо **только если вы используете упаковку или сборщики, подобные упаковке** (см. раздел \"Упакованный набор данных\") вместо стандартного дополнения.\n",
    "\n",
    " *   Если вы **создаете новые специальные токены**, теоретически, вам также следует **настроить слой эмбеддингов** (поскольку вы используете эти «пустые слоты»).\n",
    "     *   На практике ваша модель _может_ все еще работать, если вы **оставите эмбеддинги замороженными**.\n",
    "     *   Даже если представление новых токенов _случайно_ (их эмбеддинги не обучены), другие обучаемые части модели могут научиться использовать их «как есть».\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_phi = AutoTokenizer.from_pretrained(\"./phi3-mini-tokenizer\")\n",
    "config_phi = AutoConfig.from_pretrained(\"./phi3-mini-tokenizer\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2803, 29915, 29879, 5993, 675, 445, 10541, 29991], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi(\"Let's tokenize this sentence!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32011, 32064)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_phi), config_phi.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001),\n",
       " ('<|endoftext|>', 32000)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', 32000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.eos_token, tokenizer_phi.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Специальные токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<|endoftext|>', '<unk>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.all_special_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.cls_token, tokenizer_phi.sep_token, tokenizer_phi.mask_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.add_special_tokens({'cls_token': '<cls>', 'sep_token': '<sep>', 'mask_token': '<mask>'})\n",
    "tokenizer_phi.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<mask>', 32013),\n",
       " ('<sep>', 32012),\n",
       " ('<cls>', 32011),\n",
       " ('<|user|>', 32010),\n",
       " ('<|placeholder6|>', 32009),\n",
       " ('<|placeholder5|>', 32008),\n",
       " ('<|end|>', 32007),\n",
       " ('<|system|>', 32006),\n",
       " ('<|placeholder4|>', 32005),\n",
       " ('<|placeholder3|>', 32004),\n",
       " ('<|placeholder2|>', 32003),\n",
       " ('<|placeholder1|>', 32002),\n",
       " ('<|assistant|>', 32001)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Токен `EOS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '<sep>',\n",
       " 'pad_token': '<unk>',\n",
       " 'cls_token': '<cls>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token = tokenizer_phi.unk_token\n",
    "tokenizer_phi.pad_token_id = tokenizer_phi.unk_token_id\n",
    "\n",
    "tokenizer_phi.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```python\n",
    "\n",
    " # Обновление конфигурации модели для измененного токена PAD\n",
    "\n",
    " if getattr(model, \"config\", None) is not None:\n",
    "\n",
    "     model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "\n",
    " if (getattr(model, \"generation_config\", None) s not None):\n",
    "\n",
    "     model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Токен `PAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', 'left')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_phi.pad_token, tokenizer_phi.padding_side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Сборщики данных (Data Collators)\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"Сборщики данных\"**\n",
    "\n",
    " *   Вы можете указать аргумент `data_collator` в классе `SFTTrainer` (см. Главу 5).\n",
    "\n",
    " *   `DataCollatorForLanguageModeling` — это **стандартный** сборщик для класса `SFTTrainer`:\n",
    "\n",
    "     *   Он автоматически **копирует идентификаторы токенов в качестве меток**.\n",
    "\n",
    "     *   Он **не смещает метки**, поскольку это **обрабатывается автоматически моделью**.\n",
    "\n",
    "     *   Он включает полный текст (и промпт, и завершение) в качестве меток, что идеально для инструктивной настройки.\n",
    "\n",
    " *   Если вы выполняете дальнейшую тонкую настройку инструктивной или чат-модели, вы можете использовать `DataCollatorForCompletionOnlyLM` для **обучения только на ответе модели (завершении)**.\n",
    "\n",
    "     *   Он также копирует идентификаторы токенов в качестве меток, но **маскирует токены промпта, заменяя их идентификаторы на `-100`**.\n",
    "\n",
    "     *   В **одном взаимодействии** (один промпт и одно завершение) **шаблона ответа достаточно** для определения завершения.\n",
    "\n",
    "     *   При **нескольких взаимодействиях** (последовательность промптов и завершений) необходимы как **шаблон инструкции, так и шаблон ответа**, чтобы правильно идентифицировать и замаскировать токены промпта.\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720,\n",
       " {'messages': [{'content': 'The birch canoe slid on the smooth planks.',\n",
       "    'role': 'user'},\n",
       "   {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "    'role': 'assistant'}]})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "# преобразует пары промпт/завершение в диалоговые сообщения\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
    "len(dataset), dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# formatting_func = instructions_formatting_function(tokenizer_phi)\n",
    "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
    "dataset = dataset.map(lambda row: {'text': formatting_func(row)}, batched=True, batch_size=32)\n",
    "sequences = dataset['text']\n",
    "print(sequences[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(lambda row: tokenizer_phi(row['text']))\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### `DataCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_collator = DataCollatorWithPadding(tokenizer_phi)\n",
    "pad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=pad_collator)\n",
    "pad_batch = next(iter(pad_dloader))\n",
    "pad_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### А где же мои метки?\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/shift_labels.png?raw=True)\n",
    "\n",
    "\n",
    "\n",
    " <center>Рисунок 4.6 - Входные данные и соответствующие им смещенные метки</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### `DataCollatorForLanguageModeling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_collator = DataCollatorForLanguageModeling(tokenizer_phi, mlm=False)\n",
    "lm_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=lm_collator)\n",
    "lm_batch = next(iter(lm_dloader))\n",
    "lm_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### `DataCollatorForCompletionOnlyLM`\n",
    "\n",
    " ****\n",
    "\n",
    " **ВАЖНОЕ ОБНОВЛЕНИЕ**: `DataCollatorForCompletionOnlyLM` был удален в `trl` версии 0.20. Этот сборщик маскировал промпт пользователя, чтобы обучать только на завершении модели, исключая токены промпта из вычисления потерь, используя шаблон ответа для обнаружения начала завершения.\n",
    "\n",
    "\n",
    "\n",
    " В новых версиях эта логика встроена: токены автоматически игнорируются при вычислении потерь, если в объекте `SFTConfig` установлены `completion_only_loss` или `assistant_only_loss`. Хотя это проще, данный подход менее гибок, поскольку зависит от совместимого шаблона чата.\n",
    "\n",
    "\n",
    "\n",
    " Чтобы сохранить гибкость и продемонстрировать процесс маскирования, который теперь обрабатывается внутренне, я скопировал исходную реализацию `DataCollatorForCompletionOnlyLM` в `compatibility_functions.py` (импортирован в начале этой главы) и буду продолжать использовать ее для обучения только на завершении.\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         32010,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29889, 32007, 32001,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  8467,   434,   278,  9869,   304,   278,  6501,\n",
       "          7254,  3239, 29892,   366,  1818, 29889, 32007, 32000]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>' # id токена 32001\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels'][0]\n",
    "valid_tokens = (labels >= 0)\n",
    "tokenizer_phi.decode(labels[valid_tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Несколько взаимодействий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc6d73b08a24aba8ac795b9136c96aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_chat = \"\"\"<|user|>Hello\n",
    "<|assistant|>How are you?\n",
    "<|user|>I'm fine! You?\n",
    "<|assistant|>I'm fine too!\n",
    "<|endoftext|>\"\"\"\n",
    "\n",
    "dummy_ds = Dataset.from_dict({'text': [dummy_chat]})\n",
    "dummy_ds = dummy_ds.map(lambda row: tokenizer_phi(row['text'])).select_columns(['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010, 15043,    13, 32001,  1128,   526,   366, 29973,    13, 32010,\n",
       "           306, 29915, 29885,  2691, 29991,   887, 29973,    13, 32001,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  1128,   526,   366, 29973,    13,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   306,\n",
       "         29915, 29885,  2691,  2086, 29991,    13, 32000]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_template = '<|user|>'\n",
    "response_template = '<|assistant|>'\n",
    "completion_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
    "                                                      response_template=response_template,\n",
    "                                                      tokenizer=tokenizer_phi)\n",
    "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
    "completion_batch = next(iter(completion_dloader))\n",
    "completion_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How are you?\\n I'm fine too!\\n<|endoftext|>\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = completion_batch['labels']\n",
    "tokenizer_phi.decode(labels[labels >= 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Смещение меток\n",
    "\n",
    " ```python\n",
    " if labels is not None:\n",
    "     # перемещаем метки на правильное устройство для параллелизма модели\n",
    "     labels = labels.to(lm_logits.device)\n",
    "     # мы выполняем предсказание следующего токена; смещаем предсказанные оценки и входные идентификаторы на один\n",
    "     shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "     labels = labels[:, 1:].contiguous()\n",
    "     loss_fct = CrossEntropyLoss()\n",
    "     lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Упакованный набор данных\n",
    "\n",
    " ****\n",
    "\n",
    " **ВАЖНОЕ ОБНОВЛЕНИЕ**: `ConstantLengthDataset` был удален, начиная с версии `trl` 0.20. Упаковка уже токенизированного набора данных теперь выполняется функцией `pack_dataset()` из `trl.data_utils`. Возможно приблизить поведение упаковки предыдущих версий `trl`, установив `packing_strategy='wrapped'` в `SFTConfig`.\n",
    "\n",
    " ****\n",
    "\n",
    "\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"Упакованный набор данных\"**\n",
    "\n",
    " *   Упаковка **конкатенирует** последовательности и **разделяет** их на **равные по размеру блоки**:\n",
    "\n",
    "     *   **Не используются токены дополнения**.\n",
    "\n",
    "     *   Длина каждого блока не должна превышать **максимальную длину последовательности модели**.\n",
    "\n",
    " *   Упаковка изначально поддерживается `SFTTrainer`:\n",
    "\n",
    "     *   Установите его аргумент `packing` в `True`.\n",
    "\n",
    "     *   ~Он создает внутренний `ConstantLengthDataset` для обработки упаковки~ [удалено в v0.20].\n",
    "\n",
    "     *   Он использует функцию `pack_dataset()` для обработки упаковки.\n",
    "\n",
    "     *   Вы можете установить `packing_strategy` в `wrapped`, чтобы приблизить исходное поведение упаковки.\n",
    "\n",
    "     *   По умолчанию **нельзя одновременно использовать упаковку и сборщик**.\n",
    "\n",
    " *   Некоторые **сборщики могут эффективно упаковывать** последовательности:\n",
    "\n",
    "     *   В этом случае **аргумент `packing` должен быть установлен в `False`**, а сборщик выполняет упаковку.\n",
    "\n",
    "     *   `DataCollatorWithFlattening` — это эквивалент упаковки для `DataCollatorForLanguageModeling`.\n",
    "\n",
    "     *   `DataCollatorForCompletionOnlyLM` включает новый аргумент (`padding_free`), который заставляет сборщик, ориентированный только на завершение, работать подобно упаковке.\n",
    "\n",
    "     *   Некоторые модели (например, Llama, Phi, Mistral, Gemma, OLMo и некоторые другие) поддерживают эти сборщики с Flash Attention 2:\n",
    "\n",
    "       *   Эти модели используют `position_ids` для **обозначения границ** между исходными последовательностями, упакованными вместе.\n",
    "\n",
    " ****\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packed_seq.png?raw=True)\n",
    "\n",
    "\n",
    "\n",
    " <center>Рисунок 4.7 - Упакованные последовательности</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>\\nThe birch canoe slid on the smooth planks.<|end|>\\n<|assistant|>\\nOn the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\\n<|endoftext|>', '<|user|>\\nGlue the sheet to the dark blue background.<|end|>\\n<|assistant|>\\nGlue the sheet to the dark blue background, you must.<|end|>\\n<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset['text']\n",
    "print(sequences[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****\n",
    "\n",
    " **ВАЖНОЕ ОБНОВЛЕНИЕ**: `ConstantLengthDataset` был удален в `trl` v0.20, но мы воспроизводим исходное поведение ниже (класс был импортирован из `compatibility_functions.py`), чтобы сравнить его вывод с выводом новой версии, использующей `pack_dataset()` со стратегией `wrapped`.\n",
    "\n",
    " ****\n",
    "\n",
    "\n",
    "\n",
    " **ДО**: `ConstantLengthDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 351\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator = ConstantLengthDataset(tokenizer_phi, dataset,\n",
    "                                 dataset_text_field='text',\n",
    "                                 seq_length=64, shuffle=False)\n",
    "\n",
    "def data_generator(iterator):\n",
    "    yield from iterator\n",
    "\n",
    "packed_dataset = Dataset.from_generator(\n",
    "    data_generator,\n",
    "    gen_kwargs={\"iterator\": iterator}\n",
    ")\n",
    "packed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|> The birch canoe slid on the smooth planks.<|end|><|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|><|endoftext|><|user|> Glue the sheet to the dark blue background.<|end|><|assistant|> Glue the sheet to the dark blue background, you must'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = packed_dataset['input_ids']\n",
    "tokenizer_phi.decode(input_ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **ПОСЛЕ**: `pack_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 341\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_packed_dataset = pack_dataset(tokenized_dataset, seq_length=64, strategy='wrapped')\n",
    "new_packed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|> The birch canoe slid on the smooth planks.<|end|><|assistant|> On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|><|endoftext|><|user|> Glue the sheet to the dark blue background.<|end|><|assistant|> Glue the sheet to the dark blue background, you must.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = new_packed_dataset['input_ids']\n",
    "tokenizer_phi.decode(input_ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packing_flow.png?raw=True)\n",
    "\n",
    "\n",
    "\n",
    " <center>Рисунок 4.8 - Выбор правильной конфигурации для ваших данных</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Сборщики для упаковки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### `DataCollatorWithFlattening`\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/collator_flat.png?raw=True)\n",
    "\n",
    "\n",
    "\n",
    " <center>Рисунок 4.9 - Сборщик, подобный упаковке</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'labels': tensor([[ -100,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "          10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "           1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "           3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  8467,\n",
       "            434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "          32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "          29892,   366,  1818, 29889, 32007, 32000]]),\n",
       " 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_collator = DataCollatorWithFlattening()\n",
    "flat_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=flat_collator)\n",
    "flat_batch = next(iter(flat_dloader))\n",
    "flat_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 66]), tensor(38))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_batch['input_ids'].shape, flat_batch['position_ids'].max() + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### `DataCollatorForCompletionOnlyLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32010,   450, 29773,   305,   508,  7297,  2243,   333,   373,   278,\n",
       "         10597,   715,  1331, 29889, 32007, 32001,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000, 32010,  8467,\n",
       "           434,   278,  9869,   304,   278,  6501,  7254,  3239, 29889, 32007,\n",
       "         32001,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  1551,   278, 10597,   715,\n",
       "          1331, 29892,   278, 29773,   305,   508,  7297,  2243,   333, 29889,\n",
       "          3869, 29892,   298, 21478,  1758, 29889, 32007, 32000,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  8467,   434,   278,  9869,   304,   278,  6501,  7254,  3239,\n",
       "         29892,   366,  1818, 29889, 32007, 32000]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]]), 'cu_seq_lens_q': tensor([[ 0, 38, 66]], dtype=torch.int32), 'cu_seq_lens_k': tensor([[ 0, 38, 66]], dtype=torch.int32), 'max_length_k': tensor([38]), 'max_length_q': tensor([38])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = '<|assistant|>'\n",
    "completion_nopad_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
    "                                                            tokenizer=tokenizer_phi,\n",
    "                                                            padding_free=True)\n",
    "completion_nopad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_nopad_collator)\n",
    "completion_nopad_batch = next(iter(completion_nopad_dloader))\n",
    "completion_nopad_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Продвинутый уровень: BYOT (Принеси свой собственный шаблон)\n",
    "\n",
    " ****\n",
    "\n",
    " **Итог раздела \"Продвинутый уровень: BYOT\"**\n",
    "\n",
    " *   Каждый шаблон должен определять **шаблон ответа** и, в идеале, **заканчиваться токеном `EOS`**.\n",
    "\n",
    " *   Дважды проверьте токены `EOS`, `PAD` и `UNK` вашего токенизатора:\n",
    "\n",
    "     *   Токен `EOS` должен отличаться от токенов `PAD` и `UNK`.\n",
    "\n",
    "     *   Токены `PAD` и `UNK` могут быть одинаковыми.\n",
    "\n",
    " *   **Изменяйте размер слоя эмбеддингов** только в случае крайней необходимости — т.е. если все «пустые слоты» уже использованы:\n",
    "\n",
    "     *   При вызове `resize_token_embeddings()` модели используйте аргумент `pad_to_multiple_of`, чтобы обеспечить, чтобы размер оставался **кратным степени двойки**.\n",
    "\n",
    " *   Если вы не хотите создавать шаблон Jinja самостоятельно, вы можете использовать стандартный шаблон, например ChatML.\n",
    "\n",
    "     *   Пакет `trl` предоставляет функцию `setup_chat_format()`, но у нее есть недостатки:\n",
    "\n",
    "       *   Она назначает токен `EOS` в качестве токена `PAD` (вам нужно будет **исправить это вручную** после).\n",
    "\n",
    "       *   Она по умолчанию изменяет размер слоя эмбеддингов модели, даже если только для его уменьшения (хотя **вы можете избежать изменения размера**, выбрав подходящий `resize_to_multiple_of`).\n",
    "\n",
    " *   Вы можете определить и применить **пользовательский шаблон с помощью функции форматирования** вместо создания шаблона Jinja для вашего токенизатора:\n",
    "\n",
    "     *   Если вы укажете `formatting_func` в классе `SFTTrainer` (см. Главу 5), вашему токенизатору не нужен шаблон чата.\n",
    "\n",
    "     *   Тщательно выбирайте шаблон ответа:\n",
    "\n",
    "       *   Использование **обычных слов** (например, \"## Ответ:\") **может вызвать проблемы**, поскольку некоторые токенизаторы являются «контекстно-зависимыми» и могут разделить ваш шаблон ответа на несколько токенов.\n",
    "\n",
    "       *   Создание **дополнительного специального токена для вашего шаблона ответа** безопаснее, так как он будет закодирован как **один токен**.\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Шаблон чата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "print(tokenizer_opt.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **ChatML**\n",
    "\n",
    " ****\n",
    "\n",
    " [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md), сокращение от Chat Markup Language, был разработан OpenAI:\n",
    "\n",
    "\n",
    "\n",
    " _____\n",
    "\n",
    " \"*Традиционно модели GPT потребляли неструктурированный текст. Вместо этого модели ChatGPT ожидают структурированный формат, называемый Chat Markup Language (сокращенно ChatML). Документы ChatML состоят из последовательности сообщений.*\"\n",
    "\n",
    " _____\n",
    "\n",
    "\n",
    "\n",
    " Каждое сообщение должно содержать роль участника и соответствующее содержимое, как в диалоговом формате, представленном ранее. Это шаблон Jinja для ChatML:\n",
    "\n",
    "\n",
    "\n",
    " ```\n",
    " {% for message in messages %}\n",
    "   {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
    " {% endfor %}\n",
    " ```\n",
    "\n",
    " ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50272"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.config.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "pad_to_multiple_of = get_multiple_of(model_opt.config.vocab_size)\n",
    "pad_to_multiple_of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.resize_token_embeddings(len(tokenizer_opt),\n",
    "                                  pad_to_multiple_of=pad_to_multiple_of)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_tokenizer(tokenizer,\n",
    "                     alternative_bos_token='<|im_start|>',\n",
    "                     alternative_unk_token='<unk>',\n",
    "                     special_tokens=None,\n",
    "                     tokens=None):\n",
    "    eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
    "    pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
    "\n",
    "    # Токен BOS должен отличаться от токена EOS\n",
    "    if bos_token == eos_token:\n",
    "        bos_token = alternative_bos_token\n",
    "\n",
    "    # Токен UNK должен отличаться от токена EOS\n",
    "    if unk_token == eos_token:\n",
    "        unk_token = alternative_unk_token\n",
    "\n",
    "    # Токен PAD должен отличаться от токена EOS\n",
    "    # но может быть таким же, как токен UNK\n",
    "    if pad_token == eos_token:\n",
    "        pad_token = unk_token\n",
    "\n",
    "    assert bos_token != eos_token, \"Пожалуйста, выберите другой токен BOS.\"\n",
    "    assert unk_token != eos_token, \"Пожалуйста, выберите другой токен UNK.\"\n",
    "\n",
    "    # Создает словарь для токенов BOS, PAD и UNK\n",
    "    # Оставляет токен EOS таким, каким он был изначально определен\n",
    "    special_tokens_dict = {'bos_token': bos_token,\n",
    "                           'pad_token': pad_token,\n",
    "                           'unk_token': unk_token}\n",
    "\n",
    "    # Если есть дополнительные специальные токены, добавляем их\n",
    "    if special_tokens is not None:\n",
    "        if isinstance(special_tokens, list):\n",
    "            special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # Если есть новые обычные (не специальные) токены для добавления\n",
    "    if tokens is not None:\n",
    "        if isinstance(tokens, list):\n",
    "            tokenizer.add_tokens(tokens)\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jinja_template(tokenizer):\n",
    "    return (\"{% for message in messages %}\"\n",
    "            f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
    "            \"{% endif %}\")\n",
    "\n",
    "def add_template(tokenizer, chat_template=None):\n",
    "    # Если шаблон чата не предоставлен, создает шаблон ChatML\n",
    "    # используя токены BOS и EOS\n",
    "    if chat_template is None:\n",
    "        chat_template = jinja_template(tokenizer)\n",
    "\n",
    "    # Назначает шаблон чата токенизатору\n",
    "    tokenizer.chat_template = chat_template\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_of(vocab_size):\n",
    "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
    "\n",
    "def modify_model(model, tokenizer):\n",
    "    # Если новая длина токенизатора превышает размер словаря\n",
    "    # изменяет его размер, сохраняя кратность тому же значению\n",
    "    if len(tokenizer) > model.config.vocab_size:\n",
    "        pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
    "        model.resize_token_embeddings(len(tokenizer),\n",
    "                                      pad_to_multiple_of=pad_to_multiple_of)\n",
    "\n",
    "    # Обновляет идентификаторы токенов в конфигурациях модели\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_opt = modify_tokenizer(tokenizer_opt)\n",
    "tokenizer_opt = add_template(tokenizer_opt)\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|im_start|>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50266"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50265)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 512, padding_idx=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt.get_input_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '</s>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_opt.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of Argentina?</s>\n",
      "<|im_start|>assistant\n",
      "Buenos Aires.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = ds_msg['messages'][0]\n",
    "print(tokenizer_opt.apply_chat_template(messages, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Пользовательский шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "response_template = '##[YODA]##>'\n",
    "tokenizer_opt = modify_tokenizer(tokenizer_opt, special_tokens=[response_template])\n",
    "model_opt = modify_model(model_opt, tokenizer_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.formatting_func_builder.<locals>.formatting_func(examples, add_generation_prompt=False)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func_builder(response_template):\n",
    "    def formatting_func(examples, add_generation_prompt=False):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples['prompt'])):\n",
    "            text = f\"{examples['prompt'][i]}\"\n",
    "            try:\n",
    "                text += f\" {response_template} {examples['completion'][i]}{tokenizer_opt.eos_token}\"\n",
    "            except KeyError:\n",
    "                if add_generation_prompt:\n",
    "                    text += f\" {response_template} \"\n",
    "            output_texts.append(text)\n",
    "        return output_texts\n",
    "    return formatting_func\n",
    "\n",
    "yoda_formatting_func = formatting_func_builder(response_template)\n",
    "yoda_formatting_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The birch canoe slid on the smooth planks. ##[YODA]##> On the smooth planks, the birch canoe slid. Yes, hrrrm.</s>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "\n",
    "formatted_seqs = yoda_formatting_func(dataset)\n",
    "formatted_seqs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 133, 23629, 611, 31728, 13763, 15, 5, 6921, 563, 2258, 4, 1437, 50266, 374, 5, 6921, 563, 2258, 6, 5, 23629, 611, 31728, 13763, 4, 3216, 6, 1368, 28015, 22900, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt(formatted_seqs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##[YODA]##>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.convert_ids_to_tokens(50266)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Force is strong in you. ##[YODA]##> ', 'I am your father! ##[YODA]##> ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoda_formatting_func({'prompt': ['The Force is strong in you.',\n",
    "                                 'I am your father!']},\n",
    "                     add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Преимущества специальных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer_llama.pad_token = tokenizer_llama.unk_token\n",
    "tokenizer_llama.pad_token_id = tokenizer_llama.unk_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_llama.tokenize(prompt, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(prompt, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))[6:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_ds = Dataset.from_dict({'text': [prompt]})\n",
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "bad_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "bad_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=bad_collator)\n",
    "bad_batch = next(iter(bad_dloader))\n",
    "bad_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_response_template = \"\\n### Assistant:\"\n",
    "tokens = tokenizer_llama.tokenize(modified_response_template, add_special_tokens=False)\n",
    "token_ids = tokenizer_llama.encode(modified_response_template, add_special_tokens=False)\n",
    "list(zip(tokens, token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_token_ids = token_ids[2:]\n",
    "fixed_collator = DataCollatorForCompletionOnlyLM(fixed_token_ids, tokenizer=tokenizer_llama)\n",
    "fixed_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=fixed_collator)\n",
    "fixed_batch = next(iter(fixed_dloader))\n",
    "fixed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"### Assistant:\"\n",
    "tokenizer_llama.add_special_tokens({'additional_special_tokens': [response_template]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
    "special_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=special_collator)\n",
    "special_batch = next(iter(special_dloader))\n",
    "special_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Что ждет в книге \"Fine-Tuning LLMs\"\n",
    "\n",
    " Шаблоны чата — ключ к обузданию необузданных LLM-монстров и обучению их правильному общению с нами, людьми. Умелое размещение подсказок, или специальных токенов, в диалоге позволяет им научиться отвечать, когда их активирует правильное ключевое слово. Однако процедура обучения не лишена опасностей: активации, градиенты и оптимизатор — все требуют огромных объемов драгоценной оперативной памяти для выполнения своей работы. Умиротворение этих жаждущих памяти компонентов потребует как мастерства, так и усилий. Настройка цикла обучения — не для слабонервных. Не пропустите следующую сложную главу \"Fine-Tuning LLMs\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchbook] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
