{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Глава 1: Внимание к большим языковым моделям\n",
    "\n",
    "\n",
    "\n",
    " ### Содержание главы\n",
    "\n",
    "\n",
    "\n",
    " В этой главе мы:\n",
    "\n",
    "\n",
    "\n",
    " *   Кратко рассмотрим историю развития языковых моделей.\n",
    "\n",
    " *   Разберем основные элементы архитектуры Transformer и механизма внимания.\n",
    "\n",
    " *   Поймем различия между типами тонкой настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Архитектура Transformer\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/stacked_layers.png?raw=True)\n",
    "\n",
    " <center>Рисунок 1.1 - \"Слои\" Transformer, объединенные в стек</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/full_transformer.png?raw=True)\n",
    "\n",
    " <center>Рисунок 1.2 - Детализированная архитектура Transformer</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/bert_embeddings.png?raw=True)\n",
    "\n",
    " <center>Рисунок 1.3 - Контекстные векторные представления слов из модели BERT</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Внимание — это все, что нужно\n",
    "\n",
    "\n",
    "\n",
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 1.1 - Формула механизма внимания</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/translation_att.png?raw=True)\n",
    "\n",
    " <center>Рисунок 1.4 - Оценки внимания</center>\n",
    "\n",
    "\n",
    "\n",
    " ![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch1/multiple_keys_context.png?raw=True)\n",
    "\n",
    " <center>Рисунок 1.5 - Запрос к двумерным ключам</center>\n",
    "\n",
    "\n",
    "\n",
    " $$\n",
    "\n",
    " \\Large\n",
    "\n",
    " \\text{cos}\\theta = ||Q|| ||K|| = Q \\cdot K\n",
    "\n",
    " $$\n",
    "\n",
    " <center>Уравнение 1.2 - Косинусное сходство, нормы и скалярное произведение</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbe631f56484c42804ef54fab28e00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Загружаем все файлы в локальную директорию\n",
    "snapshot_download(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    local_dir=\"./phi3-mini-tokenizer\",\n",
    "    allow_patterns=[\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"added_tokens.json\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Затем загружаем из локальной директории\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phi3-mini-tokenizer\")\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "torch.manual_seed(13)\n",
    "# Создадим искусственные слои векторных представлений и проекций\n",
    "d_model = 1024\n",
    "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "linear_query = nn.Linear(d_model, d_model)\n",
    "linear_key = nn.Linear(d_model, d_model)\n",
    "linear_value = nn.Linear(d_model, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3387,   263, 20254, 10541]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Just a dummy sentence'\n",
    "input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_layer(input_ids)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проекции\n",
    "proj_key = linear_key(embeddings)\n",
    "proj_value = linear_value(embeddings)\n",
    "proj_query = linear_query(embeddings)\n",
    "# Оценки внимания\n",
    "dot_products = torch.matmul(proj_query, proj_key.transpose(-2, -1))\n",
    "scores = F.softmax(dot_products / np.sqrt(d_model), dim=-1)\n",
    "scores.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(scores, proj_value)\n",
    "context.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
